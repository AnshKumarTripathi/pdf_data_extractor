{"title": "Document Title", "author": "Document Author", "content": "Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nMachine Unlearning Doesn\u2019t Do What You Think:\nLessons for Generative AI Policy, Research, and Practice\nA. Feder Cooper\u2217\u22c61,2,3Christopher A. Choquette-Choo\u22174Miranda Bogen\u22175,6\nMatthew Jagielski\u22174Katja Filippova\u22174Ken Ziyu Liu\u22173\nAlexandra Chouldechova2Jamie Hayes4Yangsibo Huang7Niloofar Mireshghallah8\nIlia Shumailov4Eleni Triantafillou4Peter Kairouz7Nicole Mitchell7\nPercy Liang3Daniel E. Ho9Yejin Choi8Sanmi Koyejo3Fernando Delgado10\nJames Grimmelmann1,11,12Vitaly Shmatikov11Christopher De Sa13Solon Barocas2\nAmy Cyphert14Mark Lemley9danah boyd2Jennifer Wortman Vaughan2\nMiles Brundage David Bau15Seth Neel16Abigail Z. Jacobs17Andreas Terzis4\nHanna Wallach2Nicolas Papernot4Katherine Lee\u22c61,4\n\u2217First author\u22c6Lead, correspondence: {afedercooper, kate.lee168 }@gmail.com\n1The GenLaw Center2Microsoft Research3Stanford University4Google DeepMind\n5Center for Democracy & Technology6Princeton CITP7Google Research\n8University of Washington9Stanford Law School10Lighthouse11Cornell Tech\n12Cornell Law School13Cornell University14West Virginia University, College of Law\n15Northeastern University16Harvard Business School17University of Michigan\nAbstract\nWe articulate fundamental mismatches between technical methods for machine\nunlearning in Generative AI, and documented aspirations for broader impact\nthat these methods could have for law and policy. These aspirations are both nu-\nmerous and varied, motivated by issues that pertain to privacy, copyright, safety,\nand more. For example, unlearning is often invoked as a solution for removing\nthe effects of targeted information from a generative-AI model\u2019s parameters, e.g.,\na particular individual\u2019s personal data or in-copyright expression of Spiderman\nthat was included in the model\u2019s training data. Unlearning is also proposed as\na way to prevent a model from generating targeted types of information in its\noutputs, e.g., generations that closely resemble a particular individual\u2019s data or\nreflect the concept of \u201cSpiderman.\u201d Both of these goals\u2014the targeted removal\nof information from a model and the targeted suppression of information from\na model\u2019s outputs\u2014present various technical and substantive challenges. We\nprovide a framework for thinking rigorously about these challenges, which\nenables us to be clear about why unlearning is not a general-purpose solution\nfor circumscribing generative-AI model behavior in service of broader positive\nimpact. We aim for conceptual clarity and to encourage more thoughtful\ncommunication among machine learning (ML), law, and policy experts who seek\nto develop and apply technical methods for compliance with policy objectives.\nAuthors\u2019 expertise and intended audience. Our contributions require expertise in ML, law, and\npolicy. We intend for our audience to be members of all of those communities. We organized\na team of experts in each discipline. The resulting paper reflects the efforts of a large-scale\ncollaboration across academic institutions, civil society, and industry labs. We intend for this paper\nto be a standalone document: one with the necessary (and sometimes elementary) background\nto make our contributions legible to our diverse intended audience, and at the appropriate level\nof abstraction to encourage effective cross-disciplinary communication about machine unlearning.\n1arXiv:2412.06966v1  [cs.LG]  9 Dec 2024\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n1 Introduction\n\u201cMachine unlearning\u201d has recently captured public attention as a potential general-purpose\napproach for purging unwanted information from machine-learning (ML) models. It raises\nproblems of technical interest, but perhaps more significantly, machine unlearning also finds\nbroader appeal outside of technical circles for its perceived ability to support law and policy aims\n(Section 2). Since around 2016, technical experts and policymakers have invoked unlearning as\na way to operationalize compliance with an individual\u2019s \u201cright to be forgotten,\u201d with respect to\nremoving personal data from deployed models, as granted in the E.U.\u2019s General Data Protection\nRegulation (GDPR) [ 102].1Now, with the emergence of Generative AI, machine unlearning\u2019s\npresumptive mandate has expanded significantly. More and more, research papers, policy briefs,\nand media reports suggest machine unlearning as an approach for meeting a broad range of\nobjectives for both open and closed models and systems,2spanning privacy, copyright, safety,\nand more [e.g., 51, 63, 71, 75, 96, 118, 134, 140].\nUnfortunately, the fit between unlearning and policy is not so straightforward in practice. Machine\nunlearning is a set of technical methods and here, as always, there are critical gaps\u2014gaps that are\ntoo often overlooked\u2014between what technical methods do and what policy aims to achieve [ 31].\nOur goal is to provide conceptual clarity that elicits these gaps, and to encourage more thoughtful\ncommunication among ML, law, and policy experts who seek to develop and apply technical\nmethods for compliance with law and policy objectives (Sections 6 & 7). In summary:\nDeleting information from an ML model is not well-defined. First, information cannot be deleted\nfrom an ML model in the same way that it can from a database. During training, data are trans-\nformed into patterns that get encoded in the model\u2019s parameters\u2014patterns that are not directly or\neasily interpretable (Section 2). There is no way to cleanly identify, target, and delete specific, con-\ntained pieces of information from these parameters. Instead, it is possible, even if computationally\nexpensive, to train a newmodel on a dataset that does not contain problematic data (Section 4)\u2014for\nexample, a specific scientific paper on designing novel flu viruses or a specific in-copyright image\nof Spiderman. This is typically what it means to \u201cremove\u201d data from a generative-AI model in ma-\nchine unlearning, which deviates from intuitive understandings of the term. Removal applies to\ndiscrete pieces of data in the training dataset before training occurs ; it cannot target the latent patterns\nthat a trained model has learned across different data examples (Section 3). For example, there\nis no clear way to remove the more general concepts of \u201chow to synthesize a toxic molecule\u201d or\n\u201cSpiderman\u201d from a model; there is no single obvious or appropriate way to go about translating\nsuch open-ended aims to concrete tasks that can be implemented by an algorithm (Section 5).\nRemoving information from a model does not provide guarantees about model outputs. Second,\nremoving information from a model\u2019s parameters does not guarantee that this model could never\nproduce related information at generation time . Even if one removed all in-copyright images of\nSpiderman from a model\u2019s training data, this does not mean it would be impossible for the model\nto generate outputs that resemble Spiderman when put to use. Generative-AI models are impres-\nsive in part because they are able to generate novel outputs that transcend the information that is\nexactly contained in their training data. It is therefore a mistake to think that making a limited set\nof targeted changes to a model\u2019s parameters is sufficient to make promises about what types of out-\nputs that model could or could not possibly produce (Section 5). This point is further complicated\nby the fact that users can introduce information at generation time through prompts. In the context\nof machine unlearning, user prompts can even reintroduce information whose effects were previ-\nously removed from the model\u2019s parameters. Combining such a prompt with a model\u2019s reasoning\nabilities, it may be possible to produce outputs that are effectively the same as those that would\nhave been produced if an unlearning method had never been used in the first place (Section 5).\nIn general, removal on its own is often neither necessary nor sufficient to constrain model outputs\nin a controlled manner.3Instead, suppressing certain model outputs from being surfaced to users\nmay be a more appropriate area of focus for technical methods [e.g., 39]. While it is now common to\n1This paper focuses on generative-AI models, but unlearning methods originate from classification.\n2Our observations address cross-cutting issues applicable to both open and closed technology.\n3One important exception where removal may be necessary (but still not sufficient) concerns illegal content\nthat may have been included in the model\u2019s training data, such as child sexual abuse material (CSAM) [ 101].\n2\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\ninclude output suppression under the umbrella of \u201cmachine unlearning,\u201d arguably, these methods\nhave nothing to do with \u201cunlearning\u201d some information from a model; they serve as guardrails\non model and system outputs that bear more resemblance to alignment techniques (Section 4).\nEven seemingly innocuous model outputs can be put to undesirable uses. Lastly, even if\ntechnical research were to shift the focus to methods for suppressing undesirable model outputs,\nthis would not immediately provide solutions for all law and policy ends. Generative-AI systems\nare dual-use, where the appropriateness of downstream use wholly depends on context. There\nremain familiar (but fundamentally irresolvable) tensions that are inherent to highly generative,\ndual-use technologies [ 151], like the PC and the Internet (Section 7). Just as a PC could be used as\na tool to perpetrate fraud or to write the next great Broadway musical, a generative-AI system can\nsimilarly be put to malicious or beneficial uses. Further, on their own, generated outputs may be\ninnocuous or have significant legitimate uses; yet these same outputs could be pressed into service\nfor adversarial or malicious downstream uses. To greater or lesser extents, different unlearning\nmethods can remove the influence of specific training data from a model\u2019s parameters or suppress\nundesirable model outputs (Section 4). But the type of control these functions provide is localized\nto the model or system. Additional controls on downstream use would require anticipating how a\nperson or other agent might behave with outputs in a potentially infinite number of contexts\u2014none\nof which is reasonably under the purview of machine unlearning (Section 5).\nIn this paper, we explore these observations in detail, and examine their various implications for\nprivacy, copyright, and safety\u2014three areas for which machine unlearning has been suggested as\na viable approach for operationalizing compliance with law or policy requirements (Section 6).\nRather than following the well-trod path of surveying [ 82,83,109,114,117,143] or evaluating [ 81,\n83,89] existing unlearning methods, we take a step back and think conceptually about what, in\nprinciple, machine unlearning could reasonably accomplish.\nWe articulate fundamental mismatches between machine unlearning\u2014as a technical problem of\nstudy in ML research\u2014and aspirations for the broader impact that methods emerging from this re-\nsearch could have for law and policy. In contrast to common opinions in policy research [e.g., 9,63],\nwe show that machine unlearning\u2014both the entire class of methods and specific techniques\u2014\nshould not be misunderstood as a general-purpose solution for circumscribing model behavior in\nservice of broader positive impact. Unlearning methods are imperfect and may serve as only one\napproach of many that could, in some cases, contribute to addressing aspects of issues that are of\ninterest to policymakers. However, given the fundamental mismatches we identify, it is also hard\nto imagine that, even with time, technical solutions for unlearning will ever wholly achieve desired\nlaw and policy objectives. In light of these limitations, we provide recommendations on how\nML experts should focus their research and how policymakers can adjust their expectations and\nnorms concerning reasonable best efforts when using an unlearning method in practice (Section 7).\nWe organize the remainder of the paper as follows:\nSection 2. We begin with some necessary background on machine unlearning\u2014both its technical\nmotivations and evolving motivations for Generative AI from law and policy.\nSection 3. We identify different targets\u2014observed information, latent information, and higher-\norder concepts\u2014that model developers or custodians may want to address with unlearning.\nSection 4. Defining these targets (Section 3) helps us make clear which types of information a\nspecific unlearning method may apply to and which it does not. Some methods can remove\ntargeted pieces of observed information before training occurs. For Generative AI, most\nmethods aim to suppress model outputs that contain undesirable content.\nSection 5. Together, our discussion illuminates four important mismatches between unlearning\nmotivations (Section 2), targets (Section 3), and methods (Section 4). These mismatches lay\nthe groundwork for understanding how there are substantive aims that cannot, from first\nprinciples, be addressed with unlearning methods alone.\nSection 6. We examine how these mismatches (Section 5) manifest differently and exhibit various\nimplications for issues related to privacy, copyright, and safety contexts.\nSection 7. We suggest takeaways and possible future directions for ML research and AI policy.\n3\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n2 Background and Motivations for Machine Unlearning\nThe natural starting place for our discussion is to address first what machine learning attempts to\naccomplish. This will let us provide an intuition, grounded in an interpretation of the E.U.\u2019s Gen-\neral Data Protection Regulation (GDPR) that is prevalent in ML research, for why one might want\nto do unlearning to revert or change the results of this process (Section 2.1). From this intuition,\nwe provide a loose definition for machine unlearning that originates from traditional AI settings\n(Section 2.2). We then discuss evolving motivations for machine unlearning in response to the\nascendance of Generative AI (Section 2.3). These new motivations have encouraged an expanded\ndefinition for machine unlearning (Section 2.4), which we will rely on throughout the paper.\n2.1 ML research and its interpretation of the GDPR\nIn brief, machine learning is an area of computer science and engineering that uses techniques\nfrom probability and statistics to develop algorithms that produce models that encode patterns\nlearned from data. Model architectures range from simple linear models or decision trees to com-\nplicated, large-scale neural networks. In a bit more detail, we rely on the GenLaw glossary [31]:4\nMachine-learning neural-network models all contain parameters . . . . During an algorithmic process\ncalled training , these parameters are repeatedly updated based on the training data within the\ntraining dataset that the model has seen. Each update is designed to increase the chance that when\na model is provided some input, it outputs a value close to the target value we would like it to output.\nBy presenting the model with all of the examples in a dataset and updating the parameters after\neach presentation, the model can become quite good at doing the task we want it to do.\nEach training-data example in the training dataset \u201cis a self-contained piece of data, such as an\nimage, a piece of text (e.g., content of a web page), a sound snippet, a video, or some combination\nof these\u201d [ 31]. These examples can also include personal information\u2014home addresses, sensitive\ndemographic attributes, health information, personal photos, and more.\nIn some jurisdictions, individuals have rights associated with the control of their personal data.\nNotably, since its adoption in 2018, Article 17 of the E.U.\u2019s GDPR provides the \u201cRight to erasure\u201d\n(more commonly called the \u201cright to be forgotten\u201d) [ 45,102], which gives individuals rights (with\nexceptions) to demand that companies delete their personal data.5ML researchers often interpret\nArticle 17 to apply to both to the individual\u2019s data examples that have been used as training data\nand to the resulting trained models themselves [e.g., 10,24,73,74,84,90,106] (Appendix A).6This\npresents a problem because, in almost all cases, the model would not justbe trained on a specific,\nright-exercising individual\u2019s data. It would also be trained on data associated with thousands\nof others, if not many more. Wholesale erasure of a trained model, in response to one individual\u2019s\ndeletion request, would therefore likely be an extreme, over-broad interpretation of Article 17.7\nThis problem raises a natural question for ML research: rather than deleting a trained model\naltogether, is it possible to develop algorithms that can achieve more targeted removal of training\ndata from the model? In the specific context of ML research\u2019s common interpretation of the GDPR:\nis it possible to remove the influence of the right-exercising individual\u2019s data from the model,\nwithout imposing on the model controller the undue burden of the cost of retraining a new model\nfrom scratch without that individual\u2019s data examples (Section 4.1)? Machine unlearning is the\narea of ML research that attempts to address this question.\n4We reprint with permission excerpts from the GenLaw Glossary. This glossary provides definitions in\nmachine learning, law, and policy at the same level of abstraction that is intended for our audience.\n5There are several exceptions to this right, for instance, when keeping the data is in the public interest\n(e.g., the data are used to comply with a legal ruling) or for certain research purposes, etc. [ 102, Article 17(3)].\n6While this interpretation is common in unlearning papers, it is still very much up for debate. Exactly\nhow Article 17 may or may not apply to ML models is under active discussion. (See Section 6.1.)\n7Also consider that many thousands of people might make such a request, each one requiring retraining\na new model from scratch (Section 4.1). Retraining runs could perhaps be periodically batched\u2014removing\nmultiple individuals\u2019 personal data together in the same run to reduce the number of times a model is\nretrained. Even still, retraining could happen a large number of times for the same model.\n4\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n2.2 A loose definition for machine unlearning\nWhat is referred to as \u201cmachine unlearning\u201d in the technical literature actually corresponds to\na wide variety of different methods and techniques, which are loosely grouped together. For this\nreason, we will rely on a loose, intuitive (rather than rigorous) definition of machine unlearning\nthat we construct in relation to this common underlying technical motivation. In relation to\nearly research, machine unlearning is a subarea of machine learning that develops methods\nfor the targeted removal of the effect of training data from the trained model. We will soon refine\nthis definition (Section 2.4), in response to changing motivations in the field for when to use\nunlearning (Section 2.3) and how to implement it (Section 4).\nThis definition is deliberately broad, rather than prescriptive. We intentionally do not include\nspecific requirements for how certain information is \u201ctargeted\u201d or \u201cremoved.\u201d8For now, we also\nare not prescriptive about what the exact \u201ceffects\u201d are of \u201clearned information\u201d on the trained\nmodel\u2019s behavior. We will address this in more detail in Section 3, where we discuss different\ntypes of learned information that could reasonably be targeted for unlearning.\nThis definition covers a variety of methods in the technical literature. It encompasses prior\nwork from the last 10 years that has studied unlearning in clustering [ 53], classification and\nregression [ 11,42,100,122], federated learning [ 67,85], and more [ 17,146]. It also applies to the\nclassic paper by Cauwenberghs and Poggio [19], which studies the problem of unlearning in\nsupport vector machines (SVMs) under the name \u201cdecremental learning\u201d over two decades ago.\n2.3 Generative AI and evolving motivations for machine unlearning\nGiven the particularly high cost of (re)training large-scale generative-AI models, there is a devel-\noping interest to apply efficient unlearning methods in this area. However, translating unlearning\nmethods to generative-AI models exhibits some important technical challenges, since these mod-\nels differ from the more traditional ML models to which much prior work in unlearning has\napplied [e.g., 11,53]. Traditional AI settings tend to involve models that produce concise outputs\nfrom a bounded and typically fixed set, for example, classification labels like dogorcat. After\nusing an unlearning method on such a model, its outputs for a given input may change (e.g., its\nclassification may flip from cattodog), but the set of possible outputs (e.g., catand dog) generally\nremains the same. In contrast, generative-AI models produce \u201cinformation-rich\u201d [ 26] outputs\nof the same modality as their training data. The set of possible outputs is significantly more\nexpansive. For example, text-to-text models like Llama 3 [ 87] and those embedded in systems like\nClaude [4], ChatGPT [103], and Gemini [127] produce long-form text outputs.\nWith this key difference, the desired goals for what machine unlearning could achieve have also\nshifted. They have begun to expand beyond the scope of our loose definition for unlearning\u2014\nbeyond removal of the influence of training-data inputs on the trained model\u2019s parameters \u2014to also\nencompass desired effects on the model\u2019s possible generated outputs when the model is put to use .\n2.4 An expanded, loose definition for machine unlearning\nIn the context of more recent research on Generative AI, the loose definition for unlearning in\nSection 2.2 has widened in scope. In relation to these developments, we offer an expanded loose\ndefinition: machine unlearning is now a subarea of machine learning that both develops methods\nfor (1) the targeted removal of the effect of training data from the trained model and (2) the targeted\nsuppression of content in a generative-AI model\u2019s outputs. Later, we will organize our discussion\nof concrete unlearning methods in relation to this split (Section 4).\nIn other words, the scope for unlearning no longer just concerns what we, following Cooper and\nGrimmelmann [26], will refer to as back-end considerations: \u201ccharacteristics and capabilities of\nthemodel itself that directly result from its training.\u201d Unlearning also concerns front-end considera-\ntions: \u201chow the model behaves in generating outputs in response to . . . specific prompt[s]\u201d(emphasis\n8There is arguably a spectrum between targeted machine unlearning and unintentional (and undesired)\nloss of representation of information in the model, which prior work has studied in various settings, for\nexample, catastrophic forgetting [56, 116]. Our focus is the former. See also Section 5, Mismatch 2.\n5\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nback-endTrainingDataTrained model\n(a) Back-end\nfront-endTrained modelPromptGeneration (b) Front-end\nFigure 1: Both the ( a) back-end and ( b) front-end involve processes that have their own inputs\nand produce their own outputs (simplified here). This is why we use this additional terminology\nfor clarifying which inputs and outputs are under discussion. There is nothing complicated here;\nit is just shorthand to signal different aspects of the trained model at different points in time.\nadded) [ 26].9Both the back-end and front-end involve processes that have their own inputs and\nproduce their own outputs (Figure 1).10On the back-end, the training dataset is an input and\nthe trained model is an output; the back-end involves making choices for which training data to\ninclude, which training algorithm to run, etc. On the front-end, the prompt is an input and the gen-\neration is an output; the front-end includes the process of inference (i.e., producing generations),\nsystem-level filters that may prevent the processing of certain undesirable user prompts or the user-\nfacing output of certain undesirable generations (Section 4.2), etc. On the back-end, the trained\nmodel is an output ; on the front-end, the trained model is used to produce outputs . Throughout this\npaper, we will use this back-end/front-end terminology as a shorthand for distinguishing the\ndifferent points in time where unlearning is of interest, and which artifacts a particular unlearning\nmethod is intended to affect\u2014the model parameters or the model\u2019s possible generations (Section 4).\nExtending the example at the beginning of this section of an individual exercising their \u201cright to be\nforgotten\u201d under the GDPR: for a generative-AI model, the goal for unlearning would no longer\nsimply be to remove the influence of the individual\u2019s personal data from the model\u2019s parameters\non the back-end, but also to ensure that the resulting model could not produce outputs that reflect\nthat individual\u2019s personal data on the front-end. It is an appealing idea that machine unlearning\ncould serve both of these ends. Indeed, it would be remarkably convenient if machine unlearning\ncould be both an approach for mitigating the influence of problematic training data on a trained\nmodel\u2019s parameters andfor effectively moderating a generative-AI model\u2019s possible outputs. If\nso, it would also perhaps be reasonable to assume, as many researchers and organizations have,\nthat machine unlearning could, on its own, be used to solve issues related to problematic model\noutputs in a variety of policy-relevant domains: novel privacy challenges [ 12,23,73,94,148],\ncopyright [26, 43, 77, 137, 150], safety [80, 81, 86], and more.\nHowever, as we discuss below, these two back-end and front-end goals are very different in kind\n(Sections 4 & 5). Tying them together ultimately muddles what concrete unlearning methods\ncould reasonably achieve on their own for desired policy ends (Section 6). To arrive at this\nconclusion, we first need some additional language that will enable us to refine our discussion\nof what types of information machine unlearning could address.\n3 Targets for Machine Unlearning\nOur loose definition for unlearning is abstract (Section 2.4); in fact, it is so abstract that it allows\nfor an enormous number of reasonable interpretations and possible techniques that satisfy it. In\nthis section, our aim is to provide some language that can help us be more precise. Building on\nour loose definition, we now pin down useful ways to think about what a \u201cpiece of information\u201d\ncould mean\u2014what types of information one might want to target with unlearning. In the sections\nthat follow, we show that the targets we define are places where concrete unlearning methods\ncould potentially apply in practice (Section 4). We will also note that some articulated goals for\nunlearning escape these target definitions altogether, highlighting instances where unlearning\nmethods could not be applied rigorously or reliably for certain desired ends (Section 5).\n9Data can enter a generative-AI system on the back-end as training data (i.e., for pre-training, fine-tuning,\nalignment) and the front-end via prompts, generation-time plug-ins, and retrieval-augmented generation\n(RAG). We refer to Lee et al. [77, Part I] for more information on data and the generative-AI supply chain.\n10The utility of this framing becomes clear in Section 4, where we discuss concrete inputs and outputs\nof unlearning methods applied at these different stages. Using the words \u201cinput\u201d and \u201coutput\u201d would\nbe unclear, as they are overloaded with different meanings at different stages. Also note that this is a\ndifferent usage of \u201cback-end\u201d and \u201cfront-end\u201d from Internet software, where \u201cback-end\u201d refers to server-side\ncomponents like storage and \u201cfront-end\u201d refers to client-side components like a user interface .\n6\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nWe define three overarching (and, as we will see, overlapping) targets :observed information\n(Definition 1), latent information (Definition 2), and higher-order concepts (Definition 3).\nThese definitions build upon each other and become more abstract and indeterminate. After\npresenting the definitions, we discuss how this indeterminacy surfaces challenges for designing\nand implementing unlearning methods in practice.\nDefinition 1 Observed information. Data that are explicitly presented to the model during training.\nThese data serve as inputs to computations that update the model\u2019s parameters.\nObserved information includes training-data examples: the contiguous pieces of data that are\nthe base-level unit of input to model training (Section 2.1). For example, consider that the text\n\u201cSusan\u2019s phone number is 555-123-4567\u201d is included as an example in an LLM\u2019s training data.\nSince this text is used directly to train the LLM, it is observed information. Observed information\nalso captures sets of training examples, such as all examples in the overall training dataset that\nmention Susan. It also includes data contained within examples, such as just the phone number\n\u201c555-123-4567\u201d in \u201cSusan\u2019s number is 555-123-4567.\u201d\nEffective trained models generalize : the learning process instills models with complex patterns\nthat are derived from the observed information in the training data\u2014patterns that models can\napply to previously unseen information when they are put to use for inference or generation\n(Section 2.1). This learned information is latent in the training data.11\nDefinition 2 Latent information. Data that are not explicitly presented to the model during training,\nbut that can be derived or otherwise elicited from a trained model based on the patterns that the model has\nlearned during training.\nUnlike observed information (Definition 1), latent information is not literally observed in the\ntraining data. However, there are ML-based methods that claim to identify latent information and\nmake it observable in the trained model\u2019s parameters12or indirectly through a model\u2019s outputs\nwhen the model is put to use.13Latent information can include simple deductions [ 68,111]. For\nexample, given the observed information \u201cCarlos is going to Susan\u2019s house for a birthday party\nthis Thursday\u201d and \u201cSusan lives in Philadelphia,\u201d a possible piece of latent information is that\nCarlos is going to be in Philadelphia on Thursday.14This information is not literally contained\nin the training data; it is derived from relationships learned from observed information. Of course,\nlatent information can also be significantly more complex than such simple deductions. The\npower of large-scale models trained on enormous datasets [ 76] comes from their flexibility to\ncapture all sorts of latent information\u2014across observed information, across latent information,\nor across some combination of the two. Indeed, information can interact to produce sophisticated,\nhigher-order information that ML research often refers to as \u201cknowledge\u201d or \u201ccapabilities.\u201d\nDefinition 3 Higher-order concepts. Combinations of latent and observed information that manifest in\nthe model as complex and coherent abstractions, knowledge, capabilities, or skills.\nBefore giving some examples of higher-order concepts, some disclaimers are in order. Definition 3\nis not intended to suggest something particularly deep about how models organize information\nor exhibit complex behaviors. (This is not, after all, a paper about ontology or metaphysics.)\nInstead, we give a definition of higher-order concepts for convenience: to align with how the\nML technical literature tends to refer to conceptual learned representations. But it is nevertheless\nreasonable to think of higher-order concepts as complex combinations of latent information\u2014that\n11For useful models, most learning is generalization; however, models also memorize (near) exactly a\nportion of their training data [e.g., 18,26,48,77,99,108,121]. Understanding the relationship between\nmemorization and generalization is an active area of research.\n12For example, some methods identify \u201cconcept neurons:\u201d model parameters that relate to human-\ninterpretable concepts [8, 40, 52].\n13In some cases, it may be possible to identify non-exhaustively the data examples that contributed to latent\ninformation. However, currently, this is not true in general (Section 5). Eliciting information from the model\u2019s\nparameters in useful ways is one of the goals of mechanistic interpretability research [e.g., 25, 59, 97, 98].\n14Of course, just as with observed information, there is no guarantee that latent information is factually\ncorrect. (In this example, perhaps Carlos attends the party remotely over a video call.)\n7\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nthere is, loosely speaking, a spectrum of complexity for latent information (Definition 2), with\nsimple deductions drawn directly from observed information on one end, and significantly more\ncomplex patterns (often called capabilities or emergent abilities [112, 123, 126, 138]) at the other.\nThis spectrum reveals that Definition 3 is somewhat arbitrary, since it is not clear how to\ndistinguish when a piece of latent information is sufficiently complex to be considered a\nhigher-order concept. We do not attempt to draw these lines. Nevertheless, we still find it useful\nin the discussion that follows to have a target definition that lets us to refer to the unlearning\nof higher-order concepts, since this is a type of information that could reasonably be\u2014and, in\nsome cases, is claimed to be\u2014a target for machine unlearning (Sections 4, 5 & 6).\nGiven these disclaimers, we enumerate a few examples that satisfy Definition 3. A model\u2019s\nrepresentation of a \u201cperson\u201d [ 107] is a higher-order concept. \u201cPeople\u201d is also a higher-order\nconcept (perhaps generalized from latent information about relationships between different\n\u201cperson\u201ds). So, too, is the knowledge that composes concrete subjects like \u201cSpiderman,\u201d \u201cMarie\nCurie,\u201d and \u201cbasketball;\u201d knowledge of abstract ideas like \u201cjustice\u201d and \u201ctoxicity;\u201d and notions\nof \u201cartistic style\u201d and \u201cscientific phenomenon\u201d (as well as instances of particular artistic styles\nand phenomena, like \u201cCubism\u201d and \u201cgravity\u201d); and the ability to reason about the relationships\nbetween different concepts, including \u201cmathematical reasoning.\u201d\n4 Unlearning Methods and Evaluating Evidence for Their Success\nWith an understanding of the different types of information one may want to target with machine\nunlearning (Section 3), we next discuss concrete unlearning methods that aim to address them. By\nfocusing on targets, we show how unlearning in generative-AI contexts attempts to have targeted\neffects in two overarching ways. First, there are methods that, in line with the original loose\ndefinition of unlearning (Section 2.2), address the targeted removal of observed information\n(Section 4.1) on the back-end (Figure 1a). Second, in response to the shifting motivations\nfor unlearning in generative-AI contexts (Sections 2.3 & 2.4), there are methods for output\nsuppression (Section 4.2) of targeted information in a model\u2019s outputs on front-end (Figure 1b).\nOur treatment of specific unlearning methods for removal and suppression is fairly brief.15This\nis because our purpose is to provide sufficient framing that will enable us to elicit important\nconceptual gaps and limitations\u2014fundamental mismatches between unlearning motivations,\ntargets, and methods (Section 5). It is these mismatches that are the heart of our paper, and are\nrelevant for understanding misalignment with law and policy aims (Section 6).\n4.1 Methods for removal (of observed information)\nAs discussed above, one of the articulated goals for machine unlearning is to purge unwanted\ninformation from models (Sections 1 & 2.2). This is a fundamentally challenging technical problem\nbecause an ML model is not like a database. For a database, it is typically the case that specific\npieces of information can be identified, targeted, and deleted; but there is no direct analogue\nfor deleting targeted information from a generative-AI model. While each of a model\u2019s training\nexamples \u201cis a self-contained piece of data\u201d [ 31], this is not the case for how information learned\nfrom these examples is arranged in a trained model\u2019s parameters. The training process encodes\npatterns learned from training data in the model\u2019s parameters in ways that are not directly or\neasily interpretable (Sections 2.1 & 3).\nAs a result, \u201cremoval\u201d of information from a generative-AI model deviates from intuitive under-\nstandings of the term \u201cremoval.\u201d Instead, the most straightforward way one might \u201cremove\u201d16\ninformation is to replace the original model with a new model that is trained on a dataset that does\nnot contain problematic examples\u2014for example, a specific scientific paper on designing novel flu\nviruses or a specific in-copyright image of Spiderman. This process removes specific observed\ninformation (Definition 1) from a model\u2019s training dataset , instead of literally removing it from a\n15As stated previously, we deliberately do not provide an in-depth survey or taxonomy of state-of-the-art\ntechniques that are branded as machine-unlearning methods. Several groups of authors have already done\nso from different perspectives [e.g., 83, 109, 117].\n16We drop the quotation marks going forward; this is the sense of \u201cremoval\u201d that we use in this paper.\n8\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nmodel\u2019s parameters. This is, in some\u2014though, as we will see, limited\u2014senses an easier technical\nproblem to solve. Even though we cannot treat a model like a database, we can treat a training\ndataset like one: observed information can be relatively easily and directly identified, targeted,\nand removed from the dataset before training transforms this information and makes its effects\ndifficult to locate in the model\u2019s parameters.\nThe \u201cgold standard\u201d for machine unlearning. The method above is often referred to as retraining\nfrom scratch or the \u201cgold standard\u201d [e.g., 54,83,91]. It is the \u201cgold standard\u201d because the targeted\ninformation was literally never observed by the training process; by definition, it is guaranteed\nthat this specific, targeted information could not have influenced the model\u2019s parameters.\nAt first glance, this seems like a reasonable (albeit expensive) solution to the unlearning problem.\nHowever, the \u201cgold standard\u201d exhibits important limitations: it casts machine unlearning as\nproblem to be solved with respect to back-end inputs (i.e., training data) and, as a result, it does\nnot directly apply to all of the types of targets one might want to address with unlearning. In\nparticular, the \u201cgold standard\u201d does not directly apply to latent information (Definition 2) or\nhigher-order concepts (Definition 3), as these are types of information that emerge and get encoded\nin a model\u2019s parameters during training. In general, it is often not clear exactly which observed\ninformation contributes to latent information and higher-order concepts (Section 3). Targeted\nremoval of observed information from the training dataset can affect both the latent information\nand higher-order concepts that the model learning during training; however, in general, this\nrelationship is not well understood. As such, the \u201cgold standard\u201d may have an indirect effect\non these targets; however, it may not be effective with respect to ensuring unwanted information\nis not latent in the model\u2019s parameters, nor with respect to preventing unwanted information\nfrom manifesting on the front-end\u2014in the model\u2019s outputs at generation time (Sections 4.2 & 5).17\nFurther, in practice, implementing the \u201cgold standard\u201d is expensive\u2014often prohibitively expensive\nfor today\u2019s enormous models trained on enormous datasets by expending immense computing\nresources. This has motivated the development of lower-cost methods for removal of structured\ninformation in the training dataset to produce models that have similar properties to those that\nhave been retrained from scratch.\nStructural removal. Methods for structural removal make the \u201cgold standard\u201d of retraining\nfrom scratch more computationally efficient. To do so, instead of requiring the whole model\nbe retrained, these methods design custom model-training procedures that limit the amount of\nretraining that needs to be conducted to exclude targeted observed information [e.g., 11,144].18\nThere also exist methods that attempt to approximate structural removal, often by changing the\noriginal model\u2019s parameters rather than retraining from scratch. These methods often involve the\ndevelopment of algorithms that rely on mathematical theory to prove (under specific theoretical\nassumptions) that the modified model is (by some mathematical definition) \u201csimilar\u201d to a model\nthat has been retrained from scratch [ 58,74]. Of course, such approximations are not literally\nequivalent to retraining from scratch; they often involve a probabilistic guarantee\u2014not absolute\ncertainty\u2014that the targeted information has been successfully removed.19\nMost methods for (approximate) structural removal have been developed for traditional AI\nsettings, not Generative AI. There are a few methods for generative-AI contexts that have drawn\ninspiration from this work [e.g., 22,73]. However, for two overarching reasons, traditional AI\nmethods do not naturally translate to this newer setting. First, since structural-removal methods\n17This is why we put the term \u201cgold standard\u201d in quotation marks, which are typically absent in the techni-\ncal literature. These limitations also have broader implications, which we examine in Section 5, Mismatch 2.\n18For this reason, structural removal is commonly referred to as exact unlearning in the ML literature [e.g.,\n143]. Even though these methods are different from the \u201cgold standard,\u201d they retain the exact same guarantees\nof the \u201cgold standard,\u201d with respect to removing the effect of targeted observed information. We avoid the\nterm \u201cexact unlearning\u201d because it can be reasonably misunderstood to mean that such methods are able to\n\u201cexactly unlearn\u201d all types of targets. However, these methods only apply to observed information, not to\nlatent information that is encoded in a perhaps unidentifiable (i.e., unstructured) place in the model. Further,\nthese methods do not guarantee effective output suppression of targets. (See Section 5, Mismatch 2.)\n19Practical implementations do not always align with theoretical mathematical assumptions. In such\nsettings, methods may still work reasonably well empirically, but they may lose their theoretical guarantees.\n9\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\ntypically require specific training processes for the original model, they cannot be applied to\ntrained models that did not use those processes. This means that existing models that were not\ntrained with structural removal in mind, such as Llama 3 450B [ 87], cannot post hoc be made\ncompatible with these methods. Second, both structural-removal methods and methods that\napproximate them are very computationally expensive at generative-AI scale [ 83]. For both of\nthese reasons, removal algorithms are challenging to implement for Generative AI in practice.\nLater, we will discuss how these practical challenges have important implications for legislative\nrequirements around data deletion for production generative-AI systems (Section 6.1).\n4.2 Methods for output suppression\nThe majority of unlearning methods in Generative AI focus on output suppression (Sec-\ntions 2.3 & 2.4). In this setting, potentially problematic training data is observed during the\ntraining process, and there is no attempt to guarantee (with certainty or probabilistically) that this\nis not the case. Instead, output-suppression methods aim to prevent undesirable content from\nappearing in generations on the front-end, rather than attempting to remove the effects of targeted\nobserved information on the back-end. These methods tend to be more computationally feasible\nthan retraining from scratch (Section 4.1). Since they focus on model outputs, they are not limited\nto observed information; they also apply (to varying degrees of success) to latent information\n(Definition 2) and higher-order concepts (Definition 3).\nWe organize our discussion around two overarching approaches to output suppression: (1)\nmethods that make modifications to the trained generative-AI model, and (2) methods that leave\nthe model unchanged, but implement guardrails in the system in which model is embedded, in\norder to constrain the outputs that are presented to end users. Both of these approaches include a\nwide range of techniques that operate very differently from the removal methods discussed above.\n(See Section 5, Mismatch 1.) While it is now common to include output suppression under the\numbrella of \u201cmachine unlearning,\u201d arguably, these methods have nothing to do with \u201cunlearning\u201d\nsome information from a model; they bear more resemblance to alignment techniques.\nMethods that modify the generative-AI model .Some output-suppression methods modify the\noriginal model to attempt to direct the model away from being able to produce outputs that reflect\nundesirable content. These methods cover a variety of different alignment-inspired techniques\n(e.g., different types of additional training, reinforcement learning) [ 66,88,91,145,149] and model\nediting [ 93,95]. They all use back-end modifications to the trained model to try to alter the model\u2019s\noutputs at generation time on the front-end. As we have noted throughout, this is challenging\nto do in a targeted way because the relationship between model parameters and model outputs\nis not straightforward or, in some cases, possible to determine (Sections 3 & 4.1). As a result, while\nmodel-based methods for output suppression can make the generation of undesirable content\nless likely, they do not provide guarantees that the model could never produce such content.\nMethods that implement guardrails in the generative-AI system .Some output-suppression\nmethods leave the trained generative-AI model unchanged and instead take effect in the generative-\nAI system in which the model is embedded. For example, output filters may be wrapped around\nmodel outputs in order to prevent generations that contain certain undesirable content from\nbeing surfaced to end users [ 129]. This requires no change to the generative-AI model: output\nfilters operate entirely on the front-end. For example, a user may prompt the system to generate\nthe molecular formula for the smallpox virus and, in response, the model may generate that\nformula; but, the output filter may identify the formula as problematic, and not surface it to\nthe user. Similarly, a system developer could implement input filters that filter problematic\nuser prompts [ 104]\u2014e.g., a filter that flags the user\u2019s prompt to generate the smallpox formula,\nand prevents the prompt from ever being supplied as an input to the model. These filters may\nthemselves be implemented with ML models (e.g., more traditional ML classifiers), which are\nimperfect; they may attempt to target certain types of information, but may do so with greater\nor lesser degrees of precision and accuracy. Other proposed methods utilize the system prompt\nfor output suppression. A system prompt is a piece of developer-chosen text that the system\nadds internally to the context of all user-supplied prompts, often to coax the model away from\nproducing generations that contain undesirable content [ 106,129]. Such in-context mechanisms\nmay (or may not) work in practice; they are generally imprecise.\n10\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nSince all of these methods focus on suppressing outputs, their success is most often evaluated by\nexamining how they affect the types of generations that are produced in some downstream task.\nThis often involves prompting the model or system with respect to content that the method in-\ntended to suppress, and observing if the resulting generations do not reflect that information [e.g.,\n20,43,91].20For example, in safety contexts, such evaluations often rely on the WMDP bench-\nmark [ 81], which is a multiple-choice question dataset that focuses on biological, chemical, and\ncyber-security risks. One might test the original model on this question dataset as a baseline,\nand then apply an output-suppression method, re-test, and quantify changes in the answers as a\nproxy for determining if \u201cunsafe\u201d knowledge is no longer reflected in the model\u2019s answers [e.g.,\n125].21They might also perform a similar test for a model trained using the \u201cgold standard\u201d as\nanother point of comparison [ 131] (i.e., evaluate the front-end behavior of a model that has had\ninformation removed on the back-end). Beyond evaluations like these, it is also common to test\nif the application of an unlearning method has effects on information that was notintentionally\ntargeted\u2014to evaluate if metrics for overall model utility are preserved [11, 69, 74, 83].\n5 Mismatches between Unlearning Motivations, Targets, and Methods\nFour important problems emerge directly from our discussion of removal of observed information\n(Section 4.1) and output suppression (Section 4.2) above. Output suppression is not a replacement\nfor removal of observed information (Mismatch 1). Conversely, removal of observed information\ndoes not guarantee meaningful output suppression (Mismatch 2). More generally, models are\nnot equivalent to their outputs (Mismatch 3) or, relatedly, to how their outputs are put to use\n(Mismatch 4). We address each of these points in turn.\nMismatch 1 Output suppression is not a replacement for removal of observed information.\nMethods that aim to suppress certain model outputs on the front-end are intrinsically different\nfrom back-end removal of observed information from the model\u2019s training dataset (Section 4).\nWith output-suppression methods, it is possible that a target could still be represented in the\nmodel, and it is possible that this target could manifest in the model\u2019s outputs.22These details\ncould have important consequences for law and policy. For example, if a piece of legislation\nwere to call for the explicit removal of a piece of training data from a model\u2019s training data\nset\u2014to guarantee that a particular piece of information was never observed during training\u2014\nunlearning methods that fall short of a guaranteeing structural removal (Section 4.1) would likely\nnot suffice [ 50]. In other cases, modifications to the model or system to suppress certain types\nof observed information may be sufficient for some compliance requirements (Section 4.2). In\ngeneral, the appropriateness of unlearning methods for removal or suppression to operationalize\ncompliance with legislation in practice will depend on the exact details. These details include the\nparticular legal domain in question, and perhaps also the circumstances of the use that potentially\nexposes information that was meant to be addressed (e.g., if some atypical, adversarial usage\npattern is necessary for exposure of problematic observed information).\nMismatch 2 Removal of observed information does not guarantee meaningful output suppression.\nStructural removal (Section 4.1) is insufficient to suppress model outputs that bear some resem-\nblance to the removed data. Exactly removing a piece of observed information on the back-end,\nlike a particular phone number, does not guarantee that it would be impossible on the front-end\nfor a model to generate that phone number. Given latent information the model may contain\nabout other phone numbers (and about numbers in general), it may be possible for the model\n20There are various other types of evaluations, for example, probing latent information in the model. We\ndefer to Lynch et al. [89] for further discussion on evaluation strategies.\n21In practice, given the open-ended \u201cinformation-rich\u201d outputs of generative-AI models, it is very chal-\nlenging (and an open research area) to come up with methods that reliably measure properties of model and\nsystem outputs [ 136]. Evaluation benchmarks like WMDP attempt to mitigate this complexity by setting up\ntasks (in this case, multiple-choice questions) that constrain the open-endedness of generated outputs.\n22As in our discussion of shifting goals for machine unlearning (Section 2) and unlearning methods\n(Section 4), we continue to see a slippage between what model is(i.e., what is stored in its parameters) and\nthe outputs that a model could produce . We attend to this in more detail below in Mismatch 3.\n11\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nto generate a specific phone number for which all associated observed information has been\nremoved (Section 6.1). Similarly, on the back-end, one could remove all in-copyright images\nof Spiderman from an image-generation model\u2019s training dataset and retrain from scratch with\nthe hope this suffices to remove the higher-order concept of \u201cSpiderman.\u201d However, this does\nnot guarantee that, on the front-end, the new model could not possibly produce an output that\nmight be \u201csubstantially similar\u201d to copyrighted expression of Spiderman, based on how the model\ngeneralizes from latent information derived from the remaining training examples (Section 6.2).\nIn both cases, removal could perhaps make the generation of similar outputs less likely; however,\nthis cannot be assured in general (Sections 4 & 7.2).\nFrom these examples, our main point is that there is a meaningful slippage that occurs when\nemploying a removal technique (Section 4.1) in service of output suppression (Section 4.2): it is\nunclear which set of information should be targeted for removal in order to prevent the generation\nof certain outputs at generation-time. Removal of a narrow set of observed information (e.g.,\nexamples that contain phone numbers) from a model\u2019s training data can easily be under-inclusive ,\nwith respect to effectively suppressing the contents of that information at generation time. Being\nover-inclusive with how to instantiate targets for removal is also a potential problem, especially\nfor cases that attempt to handle indeterminate higher-order concepts (Definition 3). One could\nremove all information related to comic books, spiders, the colors blue and red, the humanoid\nform, etc. But this is arguably too broad: it may be more effective at preventing generations that\nreflect \u201cSpiderman,\u201d but it also removes significantly more information that one did not originally\nintend to target [e.g., 64, 89].23\nBoth sides of these examples\u2014of over-inclusiveness and under-inclusiveness\u2014further clarify how\nthe \u201cgold standard\u201d can be challenging to implement and interpret as a baseline for unlearning (Sec-\ntion 4.2). As discussed in Section 4.1, the \u201cgold standard\u201d involves retraining a model from scratch\nwith a set of examples removed from the training dataset; it applies directly to observed informa-\ntion. Using the \u201cgold standard\u201d can indirectly affect latent information or higher-order concepts,\nbut it cannot ensure removal of or prevent generations that reflect these types of information. To try\nto capture some amount of these types of information in practice, implementations of this approach\nrequire navigating difficult, if not arbitrary, trade-offs to draw boundaries around what exactly\nto include for removal. For example, one could choose to retrain without all in-copyright training-\ndata images of Spiderman that they manage to identify, but this would not necessarily include\npictures of people in Spiderman Halloween costumes (Section 6.2). How to make these choices is\nclearly not a straightforward task, and yet it is essential when evaluating a particular unlearning\nmethod against the \u201cgold standard\u201d as a baseline, in order to make judgments about its efficacy.\nMismatch 3 Models are not equivalent to their outputs.\nThe slippage discussed above\u2014between back-end, targeted removal from a model\u2019s training\ndataset and effective, front-end suppression of certain information at generation time\u2014runs deep\nin unlearning research. Notably, it is typical to evaluate the success of an unlearning method\nnot by examining changes in the model\u2019s parameters , but by prompting the model and measuring\nthe extent to which certain types of outputs are no longer generated (Section 4.2).\nThis slippage has consequences for how we should think about gauging the success of an unlearn-\ning method. For example, consider that an individual p0has associated data examples and that\na model trainer retrains a model from scratch without those examples, i.e., p0\u2019s examples have\nbeen removed on the back-end. But now consider that there are also training examples related\nto individuals p1,. . .,pnthat are by some quantitative measure similar to p0\u2019s. On the front-end,\na user prompts the retrained model with some (perhaps public) information about p0(e.g., de-\nmographics, address), with the goal of revealing information about p0\u2019s health status. Combining\nthe latent information in the model (from training on examples concerning p1,. . .,pn) with the ad-\nditional information provided in the user\u2019s prompt about p0, the model generalizes to produce an\noutput that reveals sensitive information about p0\u2019s health status. This problem is related to what\nShumailov et al. [115] calls \u201cununlearning\u201d: \u201cunlearned knowledge gets reintroduced in-context,\neffectively rendering the model capable of behaving as if it knows the forgotten knowledge.\u201d\n23It is in a sense possible to make unlearning effective by being so over-inclusive\u2014by removing or sup-\npressing so much information\u2014that the model loses its ability to produce anything useful. However, it is also\narguable that this is not a successful application of unlearning, since it is not \u201ctargeted\u201d in a meaningful way.\n12\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nPerhaps this is an obvious possible outcome: the model has not unlearned the ability to generalize\nabout p0from removing p0\u2019s data alone. (Indeed, generalization is arguably the main goal of\nmachine learning; see Section 3.) When taking front-end outputs into consideration, not just back-\nend removal of information, it is arguably the case that, for some contexts (Section 6), information\nabout p0has not been successfully unlearned in a meaningful way. That is, removing sensitive\ninformation about p0from the model on the back-end does not mean that the model could not be\nprompted to produce sensitive information about p0on the front-end (e.g., it may still be possible\nto use the model to make sensitive inferences about p0). Structural removal of only p0\u2019s examples is\nperhaps under-inclusive, when taking into consideration how the retrained model might respond\nto prompts. But removing examples related to p1, . . . , pnwould be over-inclusive (Mismatch 2).\nMismatch 4 Models are not equivalent to how their outputs are put to use.\nA corollary follows from Mismatch 3, which involves another slippage. Among those who mis-\ntakenly believe that unlearning is a standalone solution for effectively moderating possible model\noutputs on the front-end, some reason further that this could help curtail further downstream\nundesirable or malicious model uses in practice.24It is obvious, but nevertheless important, to\nemphasize that seemingly innocuous outputs could be put to undesirable downstream uses. To\ngreater or lesser extents, different unlearning methods can remove the effect of observed informa-\ntion from models or suppress certain types of model outputs; but the type of control this provides\nis localized to the model\u2019s parameters and outputs. Additional control would require anticipating\nhow a person or other agent might behave with generative-AI outputs in an unbounded number\nof contexts\u2014-none of which is reasonably under the purview of machine unlearning.\n6 Machine Unlearning in Policy and Practice\nThe mismatches (Section 5) between unlearning targets (Section 3), methods (Section 4), and\ngoals (Section 2) present clear technical and substantive challenges. We next consider how these\nmismatches manifest in specific ways and introduce complications for three law and policy areas\nwhere researchers and organizations have suggested that unlearning could help achieve certain de-\nsired ends for broader impact: privacy (Section 6.1), copyright (Section 6.2), and safety (Section 6.3).\nA common theme for these areas is the underlying assumption that using unlearning methods\nto constrain model outputs could potentially act in the service of more general ends for content\nmoderation\u2014to prevent users from generating potentially private, copyright-infringing, or unsafe\noutputs. For each, bringing in domain-specific details amplifies the mismatches that we describe in\nSection 5, revealing an even deeper disconnect between the use of unlearning methods in practice,\nactual policy considerations, and regulatory compliance. To address this disconnect, judges\nand policymakers will need to set reasonable expectations concerning the imperfect outcomes of\nbest-effort implementations of unlearning methods to support specific policy goals (Section 7.2).\n6.1 Privacy\nGiven the breadth of data generative-AI models ingest for training, many experts worry about\nmodels revealing private information that they were trained on through their generations [e.g.,\n9,12,31,94,99,120]. These concerns relate to privacy rights in different jurisdictions and\nassociated remedies to preserve those rights. As discussed above (Section 2.1), in a number of\njurisdictions, individuals have the right to request that organizations delete their personal data,\nalso referred to as the \u201cright to be forgotten,\u201d following Article 17 of the GDPR [102]. Regulators\nmay seek remedies that require the removal of a set of data examples used for model training\n(Section 4.1) that they assess to have been unlawfully or improperly collected. In other cases,\nremedies may be more far-reaching; regulators may seek to delete a trained model in its entirety,\nwhich is often referred to as algorithmic disgorgement [1, 77, 141].\n24Similar observations have been made in algorithmic-fairness contexts: a model that produces risk scores\nfor criminal recidivism is distinct from the distribution of scores that model produces over a given population,\nwhich is again distinct from how the (distribution of) scores gets used for decision-making, e.g., a magistrate\nusing those risk scores to inform their judgments about whether or not to grant a defendant bail upon\nrearrest [e.g., 7, 32]. Nevertheless, this slippage takes on an expanded meaning for generative-AI contexts.\n13\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nIn the context of data protection and privacy of personal information, deletion requirements25also\noften demand removal of data within a certain time limit. For example, the California Consumer\nPrivacy Act (CCPA) requires businesses to reply to data-deletion requests within 45 business days,\nextendable to 90 business days [ 14]. While deleting specific records from a traditional database or\ndataset in this time frame is often technically feasible, some laws recognize that, in other cases,\ndeletion may be less feasible or require \u201cdisproportionate effort.\u201d In such circumstances, some\njurisdictions may provide exceptions to deletion requirements, for example, in cases where the\ninformation is otherwise publicly available, or is necessary to complete a transaction, achieve\npurposes in the public interest, or comply with other legal obligations [e.g., Article 17(3), 102].\nSome jurisdictions have also ruled that information should be suppressed from being presented to\nusers, even if the underlying data could not be deleted [39].\nSuch requirements and possible remedies have motivated attention to methods in machine\nunlearning as an approach for achieving compliance with privacy legislation. For example, at least\nin principle, unlearning perhaps seems like a direct match for satisfying data-deletion requests\nin a more efficient and targeted way: unlearning methods could be a finer-grained alternative\nto complete model disgorgement, less expensive and more efficient than retraining models from\nscratch on new datasets, and, due to improved efficiency, more suitable for satisfying deletion\ntime frames generally required by privacy and data protection laws. More generally, unlearning\nmethods have appeal because they seem to strike a balance between desires to enable large-scale\ntraining of AI models and to retain a toolkit of interventions that advance privacy. But of course,\nas with assessing any potential remedy, it would likely be necessary to consider the feasibility\nor reasonableness of using a particular unlearning method in practice, with respect to desired\ntargets, costs, and overall effectiveness.\nWe address some of these considerations below, organized around three broad goals that we\nobserve for unlearning-related efforts for Generative AI that pertain to privacy concerns grounded\nin regulatory frameworks: (1) data deletion (i.e., removing observed information from a model\u2019s\ntraining dataset), as well as suppression at generation time of (2) outputs that resemble personal\ninformation and (3) latent information.\nData deletion requests (i.e., removal of observed information). Data deletion involves entirely\nremoving observed information (Definition 1) from training datasets. It is often motivated by\n(1) legal rights of individuals (often called data subjects ) to request the deletion of personal data\ncoupled with (2) implicit expectations that removing observed information will help to mitigate\nagainst models outputting verbatim pieces of potentially private training data.\nIn some cases, for Generative AI, data deletion is most straightforwardly implemented by re-\ntraining a model from scratch or, if applicable or feasible,26some structural-removal method\n(Section 4.1) with the right-exercising data subject\u2019s examples removed from the training dataset.27\nDepending on the reason for deletion, this might on its own suffice for certain deletion requests.\nFor example, if the main issue being remedied is lack of consent for the use of a data subject\u2019s\npersonal data, output suppression might not be a relevant remedy. It also may not be an issue\nif inferences can still be made about the data subject, so long as those inferences are based on\ntraining and prompt data that have been processed with proper consent.\nHowever, even though removal methods seem like a direct match for implementing data deletion,\nthey are not a straightforward solution in practice. In general, there are significant technical\nchallenges in identifying all instances of observed information that meet certain privacy-relevant\n25Other legislation, e.g., the Virginia Consumer Data Protection Act [ 135], also has requirements for data\ncorrection, not just data deletion. Correction could be operationalized as removal of the incorrect data and\nreplacing them with (i.e., retraining with) the corrected data.\n26Recall that structural-removal methods are not currently widely usable for generative-AI contexts.\nMethods that approximate structural removal do not guarantee that the targeted observed information is\nactually removed. (See Section 4.1 & Section 5, Mismatch 1.)\n27Separately, some argue that models trained with methods like differential privacy are sufficient to\npreserve a data subject\u2019s privacy; in such cases, some believe that the data subject\u2019s privacy is retained\neven though their examples are included in the training data, so it would not be necessary to use machine-\nunlearning methods to remove them. See Brown et al. [12] for additional discussion.\n14\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\ncriteria within large-scale datasets.28Consider a deletion request to remove images of a particular\ndata subject from the training data used to produce an image-generation model. This may be\ncomputationally expensive at scale and require the use of ML tools that are themselves imperfect\nat identifying all such examples. Even if we had tools to guarantee perfect identification of a given\nset of examples, in privacy contexts, there are fundamental challenges for drawing boundaries\naround what this set ought to include (Mismatch 2). Should the set to remove be conservative and\ninclude images that only feature the right-exercising data subject? Should it be more (and perhaps\noverly) inclusive, and also cover family photos where other data subjects are also present? Photos\nwhere the right-exercising data subject is in the background?29\nFurther, even if perfect removal of observed information were to satisfy deletion requests in name,\nthis would not guarantee that a model could not produce outputs that reflect these data, which\ncould matter in contexts where model outputs are also of concern. Even in the best-case scenario,\nan unlearning method that satisfies privacy requirements on the back-end (i.e., with respect to\nhow models are trained) would be insufficient to guarantee privacy is preserved on the front-end\n(i.e., at generation time) (Mismatches 2 & 3). In general, on their own, methods for removal are\ninsufficient to guarantee privacy is preserved on the front-end.\nOutput suppression of observed information (and information that resembles it). Stakeholders\nmay also focus efforts on suppressing certain pieces of observed information from outputs, either\nthrough modifying the generative-AI model (e.g., with RLHF) or system-level filters (Section 4.2).\nThis would cover cases in which a particular piece of observed information was for some reason\nnot included in a deletion request (e.g., because there was a failure to identify or deem it appropri-\nate for deletion), as well as cases in which latent information in the model (Definition 2) enables\nthe generation of outputs that resemble the right-exercising individual\u2019s personal information.\nOutput suppression would prevent surfacing observed information to end users, but would\nnot actively remove it from models or training datasets. This approach most closely resembles\nthe notion of the \u201cright to be forgotten\u201d that follows from the Court of Justice of the European\nUnion (CJEU) ruling in 2019. The CJEU ruled that Google should act on requests from data\nsubjects by suppressing information from a viewable index in relevant jurisdictions, but did not\nnecessarily require deleting that information from underlying data storage [ 39].30Approaches\nthat support suppression of certain types of outputs are imperfect (Section 4.2); it would be\nlikely that efforts to suppress observed information would be subject to a test of reasonable or\nproportionate effort, with effectiveness determined by an evaluation of how difficult it would\nbe to extract the suppressed observed information from the model following the application of\ntechnical or procedural interventions, for example, through red teaming and related procedures.31\nOutput suppression of latent information. Additionally, many privacy practitioners have come\nto recognize that simply restricting the collection or processing of certain observed information\nmay not mitigate privacy concerns. This can be the case if technology enables an actor to infer\ninformation about a particular data subject based on latent information derived from similar data\n28Current legislative deletion provisions tend to have concrete scopes for deletion criteria, such as deletion\nof data associated with a particular user account [e.g., 14]. Such boundaries are less clear for training datasets,\nfor which the underlying training examples tend not to be organized in relation to their provenance [ 76], e.g.,\naccording to the user to which the data relate.\n29Methods that approximate structural removal (Section 4.1) inherit these challenges. Unlike structural-\nremoval methods, they do not guarantee with certainty that the chosen set of examples is removed from the\nmodel (Mismatch 1). The extent to which more efficient, but less accurate, approximate-structural methods\ncould be sufficient to stand in for structural ones is a question for policymakers and regulators [28].\n30In brief, the court found that the request in question fell under the law [39, paragraph 52], that removal\nof the information from all domains (not just those that reflect the Member States of the European Union)\nwas an over-broad interpretation of the authority and scope for the relevant laws [ 39, paragraphs 59-65], and\nthat it would suffice to de-reference the information \u201con the versions of that search engine corresponding\nto all the Member States, using, where necessary, measures which, while meeting the legal requirements,\neffectively prevent or, at the very least, seriously discourage an internet user conducting a search from one of\nthe Member States on the basis of a data subject\u2019s name from gaining access, via the list of results displayed\nfollowing that search, to the links which are the subject of that request\u201d [39, paragraph 74].\n31For discussions of red teaming, we refer to Feffer et al. [46] and Chouldechova et al. [21].\n15\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nsubjects who have consented to (or not objected to) data processing (e.g., inferring p0\u2019s health\nstatus in Mismatch 3), or to infer sensitive characteristics from benign data that may be subject\nto fewer restrictions. For instance, California privacy regulators view such inferred information\nto still be personal information about consumers over which they can exercise their rights, when\nsuch information is used to make a profile about them [14].\nStakeholders may be concerned about two distinct elements of inferences like these that are derived\nfrom latent information. First, if a generative-AI model or system has explicitly generated and then\nstored inferred information about an individual, such that a new data point has been explicitly cre-\nated (e.g., a data point about p0\u2019s inferred health status), deleting that new data point from storage\nmay be expected in order to meaningfully preserve that individual\u2019s privacy. However, if a model\nhas the capability to draw connections about a data subject via latent information in its parameters\nand additional information provided in the user\u2019s prompt, output-suppression approaches may\nbe more appropriate to prevent generations that compromise that data subject\u2019s privacy.32\nImportantly, all three areas discussed above are neither mutually exclusive nor independent. They\ncould each be implemented in the service of satisfying privacy aims. But this is also not straight-\nforward, as sometimes different privacy goals and the relevant technical approaches to attempt to\naccomplish them may be in tension. For instance, implementing an output-suppression interven-\ntion may require a system operator to retain the information that should be prevented from being\nsurfaced, in order to filter out this information from an output or filter out prompts that aim to\nsolicit it.33Removal of observed information, meanwhile, may create the (as we have seen, false)\nimpression that a model will not be able to produce a specific piece of information. Failing to imple-\nment efforts to prevent the generation of that information may lead to similar concerning impacts\nof the collection or retention of that data. Lastly, just as it is challenging to draw clear boundaries\naround which data to remove to satisfy a deletion request, it is similarly a difficult and open-ended\nproblem to draw boundaries around what to suppress from model and system outputs.\n6.2 U.S. Copyright\nAt first glance, as part of a response to claims in the U.S. that allege copyright infringement in\nconnection with generative-AI models and systems, it may seem appealing to attempt to use\nmachine-unlearning methods to target higher-order concepts (Definition 3) that relate to creative\nexpression, as perhaps a way to operationalize notice-and-takedown requests [ 38].34However, U.S.\ncopyright is not a straightforward problem, and unlearning is not a straightforward solution.35\nWe begin with some brief background on U.S. copyright law. Copyright law protects \u201coriginal\nworks of authorship fixed in any tangible medium of expression\u201d [ 33]. This means that copyright\nprotection extends to a particular image or a particular paragraph of writing, but not to any\nideas or facts contained in it. Because copyright law gives creators the exclusive right to prepare\nreproductions (copies) and derivative works, courts examine whether potential copies are\n\u201csubstantially similar\u201d to the original work, and whether those copies thus infringe on the\nrights of the copyright holder. Substantial similarity is a challenging concept with a varied and\ncomplicated history in copyright caselaw. Common tests to determine substantial similarity are\nsubjective [ 110]; judgments for substantial similarity cannot \u201cbe reduced to a simple formula that\ncan easily be applied across different works and genres\u201d [77, p. 72].\n32As in Section 3, the factuality of a piece of latent information is not relevant for our purposes. For\nexample, making an incorrect inference about p0\u2019s health status may still violate p0\u2019s privacy. More generally,\nsuch false or \u201challucinated\u201d outputs can still cause harm.\n33This tension\u2014of needing to retain information in order to facilitate suppression\u2014is also relevant for\ncopyright (Section 6.2) and safety (Section 6.3). More generally, this tension pre-dates interest in machine\nunlearning for Generative AI. For instance, in the past, Facebook attempted to address the spread of NCII on\nits social-media platform by requesting users to upload the images in question to another Facebook-hosted\ntool, so that Facebook could identify and remove the images from the platform [62].\n34See Lee et al. [77, Part II.G] for more detailed discussion on Section 512 and Generative AI.\n35We limit our specific discussion to U.S. copyright. Other jurisdictions exhibit differences in copyright\ndoctrine and caselaw, for example, with respect to exceptions to copyright-holders\u2019 exclusive rights. While we\ndraw from U.S. doctrine and caselaw, the overarching points that we make in this section about unlearning,\nsubstantial similarity, and causation have broader relevance.\n16\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nWhen one looks at the processes that go into the training, deployment, and use of generative-AI\nsystems, there are several places where a \u201ccopy\u201d could be made, e.g. copying the data examples\nin a training dataset within a program, training a model, and generating substantially similar\ncopies of the data examples at generation time. Not all such copying is copyright infringement.\nDepending on the circumstances, the defense of \u201cfair use\u201d may protect uses of a work for a\ndifferent purpose (potentially including training a model) and some uses that modify the work\nin \u201ctransformative\u201d ways. Both \u201cfair use\u201d and \u201ctransformative use\u201d are also technical terms in\ncopyright [ 34,79]. Whether a particular use is fair depends on the facts of each case, including\nthe effect on the market for the copyrighted work in question. But courts in the most analogous\nsituations have held intermediate copies made for purposes of generating new output (e.g.,\nfor model training) to be fair use [ 6,78]. However, they are less likely to hold the output of a\ngenerative-AI model or system is fair use if it is substantially similar to the original\u2014unless it\nparodies or comments on the original [60, 77].\nFollowing from this brief background, we focus our discussion of copyright and machine un-\nlearning with respect to training-data inputs on the back-end and generated outputs on the\nfront-end. We do not address potential implications for intermediate artifacts, e.g., a trained\nmodel\u2019s parameters [26].\nSuppression of substantially similar outputs. If a model generates an output that is substantially\nsimilar to a copyrighted work, in response, it may be tempting to use machine-unlearning methods\nto remove the ability to do so. For the reasons discussed above concerning output-suppression\nmethods (Section 4.2), this is challenging for each unlearning target because there is no notion of\nsimilarity that can be used to programmatically and comprehensively determine which works are\nsubstantially similar in the interest of copyright law [ 77,110]. A feature, like a color scheme, may be\nproblematic if copied from one work but not from another work (so long as it is copyrightable sub-\nject matter). The techniques used to suppress generations similar to a particular in-copyright image\nof Mickey Mouse may not generalize to suppressing generations that are similar to another image\nof Mickey Mouse\u2014or of any other Disney character [ 77, Part II.F]. That is, while such techniques\ncould prevent potentially problematic outputs in some cases, they cannot generalize to all cases.\nFurther, in order to suppress certain outputs, for example, those that resemble in-copyright\nexpression of \u201cSpiderman,\u201d the overall generative-AI system likely needs to have learned\ninformation about this expression in order to notpresent it to the end user. For example, an output\nfilter would need to be able to identify \u201cSpiderman\u201d (likely, from being trained on data that\ncontain \u201cSpiderman\u201d-related expression) in order to filter it out. So, even if one were to remove\nall instances of \u201cSpiderman\u201d from the generative-AI model , more generally, it might be infeasible\nto remove all information about \u201cSpiderman\u201d from the generative-AI system ; such information\nmight be required to effectively implement output suppression at the system level.\nRemoval of specific training examples. If one were to remove a particular data example from\nthe model, by contrast, this is a direct application of removal of a piece of observed information\n(Section 4.1). However, more generally, removal still might not always be the appropriate\napproach; it could constitute over-reach, since copyright does not forbid all forms of copying\n(e.g., fair uses, internal copies [ 57]). And removal could also likely be overbroad, as discussed\nin Mismatch 2, but with particular implications for copyright. Removal of a data example\ncould prevent transformative, non-infringing uses of the data example in addition to potentially\ninfringing ones. None of the unlearning methods we have described can or do distinguish between\ntransformative fair uses and non-transformative superseding uses,36and transformativeness is\nnot the only relevant factor for fair use. It is unreasonable to expect unlearning methods to capture\nthese nuances. As evident from caselaw, courts themselves struggle to draw the line of fair use.37\n36A superseding use is when, in the market for an original work, a new work replaces an original work\n(e.g., purchases of a fourth edition of a textbook replace the third edition). A non-transformative superseding\nuse is a superseding use in which the new, replacing work does not change the character or purpose of\nthe original work (e.g., a freely circulated digital PDF of a textbook dramatically changes the market for\nfor-purchase hard copies of the textbook) [16].\n37This challenge extends beyond unlearning methods. Distinguishing between transformative fair uses\nand non-transformative superseding uses requires context (e.g., how will the generation be used?) that is\ntypically not currently available in generative-AI systems.\n17\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n(a) Image from the training dataset\n (b) Generation for the prompt \"Mickey Mouse\"\nFigure 2: CommonCanvas is a research tool and text-to-image model [ 55], trained only using\nimages with Creative Commons licenses. One can think of this model as a \u201cgold-standard\u201d base-\nline that does not contain in-copyright images of Mickey Mouse: the only examples in the training\ndata that reflect the higher-order concept of \u201cMickey Mouse\u201d are from personal photographs, e.g.,\n(a) (redacted for privacy). Even without unlicensed, in-copyright training examples of Mickey\nMouse, the model can generate outputs that resemble \u201cMickey Mouse,\u201d e.g., ( b).\nOn the front-end at generation time, unlearning may also be ineffective in a variety of cases. As\nwe have discussed throughout the piece, it is possible to run the \u201cgold standard\u201d for machine\nunlearning\u2014to retrain a model from scratch without a specific piece of observed information\n(Section 4.1)\u2014and still generate an output that is similar to that information or is otherwise similar\nto some higher-order concept reflected in that information (Mismatch 2). In such cases, this can\nbe due to the presence of elements of the original work in other works in the training data, for\nexample, related works, duplicates, or otherwise similar works that themselves may or may not be\ndeemed infringing copies of the original work. For a concrete example, consider CommonCanvas,\na text-to-image generation model, for which the training dataset\u2019s images all have Creative\nCommons licenses [ 55]. The training dataset does not contain reproductions of unlicensed,\nin-copyright images of Mickey Mouse; and yet, based on inclusion in the training dataset of\nlicensed personal photographs (e.g., from Disney World), it is still possible for CommonCanvas\nto generate images that could be judged substantially similar to \u201cMickey Mouse\u201d (Figure 2).38\nUnlearning methods as tools for causation. We next consider the use of a machine-unlearning\nmethod as a tool for determining causation in a copyright infringement suit. If a generated\noutput is alleged to be too similar to a particular plaintiff\u2019s creative work, the plaintiff (e.g.,\nan artist) will have to prove copying to establish copyright infringement. Defendants (e.g., a\ngenerative-AI company) may attempt to use a counterfactual argument to challenge causation.\n(Indeed, this is the same type of counterfactual that the \u201cgold standard\u201d for machine unlearning\nattempts to establish. See Section 4.1.) For instance, consider that a model generates an output\nthat is substantially similar to the plaintiff\u2019s work, which was included in the model\u2019s training\ndataset; if a model had been trained without the inclusion of the plaintiff\u2019s work, would the\nmodel\u2019s generated output still be the same or very similar to the substantially-similar, potentially\ninfringing output in question? If so, that may seem to suggest that the presence of the plaintiff\u2019s\nparticular work in the training dataset for the original model did not cause the output. This\nis significant in copyright law because independent creation [ 47] is a defense to copyright\ninfringement claims [ 113]. In this case, for example, the \u201cgold standard\u201d of retraining the model\nfrom scratch could be used to produce the counterfactual model without the plaintiff\u2019s work.\nThis reasoning is tempting but incorrect. Whether the counterfactual model was trained without\nthe plaintiff\u2019s work is remarkably hard to assure. This, again, is because there may exist other\nderivative works of the plaintiff\u2019s work in the training data (e.g., see Figure 2). Those derivative\nworks may have reasonable fair-use arguments. The converse, if a plaintiff can show that the\n38Here, the training process still has access to images that contain information related to \u201cMickey Mouse,\u201d\neven if those images are not exact copies of unlicensed, in-copyright images of Mickey Mouse. Access to\na copyrighted work is one type of evidence for proving copying in a copyright infringement suit [5].\n18\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\noutput would have been significantly different without the plaintiff\u2019s work, is perhaps more\nconvincing, but also flawed in practice. This is because the model training process (Section 2) is\ninherently non-deterministic.39Two models trained on the same dataset (let alone different ones)\nmay generate significantly different sets of outputs for the same prompt. Judging whether these\nsets of outputs are meaningfully (and perhaps subtly) different is not a straightforward task\nto evaluate\u2014neither with technical tools in machine learning [ 136] nor with respect to making\njudgments about similarity for copyright [26].\nIn all, these difficulties show that, while unlearning may seem appealing for copyright remedies,\njudges and practitioners must be careful to consider their current capabilities and limitations. Our\ndiscussion in this section shows that unlearning with current techniques will not map perfectly\nto the contours of copyright law. Output-suppression methods could be deemed acceptable if\ncourts accept the empirical evaluations of these methods; but judges who consider unlearning as a\nremedy to copyright infringement will have to weigh the practical limitations of unlearning meth-\nods, as well as the potential unexpected consequences of unlearning on unrelated content. That is\nparticularly true because copyright law imposes significant penalties for noncompliance, including\nstatutory damages [ 37], destruction of infringing artifacts [ 36], and even criminal sanctions [ 35].\n6.3 Safety\nLast, we address concerns about AI safety, which span a wide range of issues and communities [e.g.,\n2,3,9,13,15,27,29,44,61,105,124,130,132,139,142,147]. Among this variety, there is one\nrecurring theme that is especially important to address in relation to machine unlearning: the\nconcern that \u201cdual-use,\u201d large-scale generative-AI models exhibit\nhigh levels of performance at tasks that pose a serious risk to security, national economic security,\nnational public health or safety, or any combination of those matters, such as by . . . substantially\nlowering the barrier of entry for non-experts to design, synthesize, acquire, or use chemical, biological,\nradiological, or nuclear (CBRN) weapons [130].\nWe draw the quote above from the U.S. Executive Order on the Safe, Secure, and Trustworthy De-\nvelopment and Use of Artificial Intelligence; however, similar concerns and language can be found\nin a variety of legislative and policy documents, including the E.U. AI Act [ 44, Recital 110], the\nInternational Scientific Report on the Safety of Advanced AI produced by the AI Seoul Summit [ 9,\nChapter 4], and OpenAI\u2019s Preparedness Framework [ 105]. Generative-AI models and systems\n\u201care sometimes called \u2018dual-use\u2019 because of their potential for both benefit and harm\u201d [133].\nSome researchers and policymakers claim that, to limit potential harmful uses, machine-\nunlearning methods could be used to remove \u201cunsafe,\u201d \u201chazardous,\u201d or otherwise \u201cundesirable\nbehaviors\u201d from generative-AI models [e.g., 9,81,83,86,89,149]. For one notable example, in\nthe cross-stakeholder AI Seoul Summit report, Bengio et al. claim that \u201c\u2018Machine unlearning\u2019 can\nhelp to remove certain undesirable capabilities,\u201d e.g., those \u201cthat could aid malicious users in\nmaking explosives, bioweapons, chemical weapons, and cyberattacks\u201d [9, p. 75].\nUnclear boundaries for removal. For now, we set aside questions of output suppression, and note\nthat there are particular challenges for safety contexts with respect to drawing lines around what to\ntarget for removal from a model (Mismatch 2). For example, some proponents of unlearning as an\napproach for improving safety assume that specific topics with dual-use potential, such as synthetic\nbiology or molecular generation, can be successfully targeted for removal. But topics like these\nare broad and under-specified, and relate to all sorts of observed information, latent information,\nand higher-order concepts (Section 3). How can such topics be adequately translated into specific\nobserved-information targets to remove? How should one go about determining reasonable bound-\naries for which observed information should be kept and which should be targeted for removal?40\n39One source of non-determinism is randomness in the order in which examples are surfaced to the model\nduring training. Example ordering has an effect on the ultimate trained model\u2019s parameters and the models\noutputs [30, 41].\n40For example, even after unlearning unsafe information related to biohazards to reduce unsafe question-\nanswering capabilities, researchers have shown it is possible to recover such capabilities by further training\nthe model on unrelated benign information. The boundary around which observed information (regardless\nof whether it is considered \u201csafe\u201d or \u201cunsafe\u201d) contributes to these unsafe capabilities is unclear [152].\n19\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nIn certain cases, it may be possible to remove observed information that is intrinsically harmful or\nhas the high potential to be put to harmful uses\u2014for example, respectively, observed information\nfor non-consensual intimate imagery (NCII) [ 101] or the molecular structure of the smallpox\nvirus.41However, many types of observed information (Definition 1) do not fit into these\ncategories. Instead, potential safety issues come about from latent information (Definition 2): the\nfact that many potentially dangerous items can be assembled using observed information that\nis itself innocuous or has significant legitimate uses. For instance, from all of the information\nin a high-school chemistry curriculum, it is possible to derive formulas for toxic molecules. But\nremoval of all knowledge of high-school chemistry from a model to foreclose the possibility of\nsuch latent information is likely overbroad.\nSo far, we have only considered information on the back-end that pertains to the trained model;\nsafety challenges are also difficult when we consider the front-end. As discussed in Section 2.3\nand with respect to Mismatch 3, the open-ended format of user inputs to generative-AI models\nmeans that, via their prompts, end-users can introduce additional information into the model\u2019s\ncontext at generation time. This information could be otherwise absent from the model\u2019s training\ndata and not reflected in the model\u2019s parameters. It could also overlap with or reflect observed\ninformation that was removed from the model using an unlearning method. By bringing this\ninformation back into the generative-AI system via the prompt, the model can still be used on the\nfront-end to reason about the information it has unlearned; its output might even be the same as if\nan unlearning method had not been applied in the first place (Section 5, Shumailov et al. [115] ).42\nInherent tensions for unlearning in dual-use systems. Separate from the practical difficulties\ndiscussed above, there is an even more fundamental challenge originating from the inherent nature\nof dual-use systems. By definition, dual-use systems can be put to potentially beneficial or poten-\ntially harmful uses [ 133]. It is not just the case that innocuous observed information could lead to\npotentially unsafe latent information in the trained model; it is also possible for generated outputs\nthat are innocuous in isolation to be put to unsafe or otherwise undesirable downstream uses.\nConsider the example of unlearning all information for \u201chow to synthesize a toxic molecule,\u201d first\nintroduced in Section 1. Setting aside the tractability of translating this into concrete targets to\nunlearn, knowledge of how to actually produce such a molecule is not a property of the model in\nisolation. The ability to actually synthesize it also depends on the knowledge of the user [ 119].43\nThe particular user is clearly an important factor to consider with respect to downstream use.\nWhat if the user already has a recipe for making such a molecule (obtained from another source),\nand the generative-AI model lowers the barrier for creation of the molecule for the user by\nexplaining, in detail, how to understand the details and nuances of the recipe that they do not\nunderstand on their own? What if the model provides a single \u201cmissing piece\u201d of information\nthat is innocuous on its own (e.g., details of a single chemical reaction) that, in combination with\neverything else this user knows, enables them to create the molecule?44\n41Of course, a formula for a molecular structure is not sufficient to produce a molecule (Mismatch 4); but\nfor sufficiently dangerous molecules, the formula itself might be considered a safety risk.\n42Evaluations for the success of unlearning methods in safety contexts often do not explicitly test for this\nissue (Section 4.2). Many such evaluations rely on the WMDP benchmark [ 81], which is a multiple-choice\nquestion dataset that focuses on biological, chemical, and cyber-security risks. Setting aside the observation\nthat multiple-choice questions may not in general be the most effective way to measure such risks, this\nevaluation setup does not allow for the type of more open-ended reasoning that this scenario presents.\n43It is perhaps for this reason that OpenAI\u2019s Preparedness Framework categorizes CBRN risks in relation\nto both the model and the users. For example, this framework considers high risk to mean that the \u201cModel\nenables an expert to develop a novel threat vector OR model provides meaningfully improved assistance\nthat enables anyone with basic training in a relevant field (e.g., introductory undergraduate biology course)\nto be able to create a CBRN threat\u201d [105, p. 9].\n44One might critique this example for not qualifying to \u201csubstantially lower[] the barrier of entry for\nnon-experts\u201d to perform unsafe actions in the world. Perhaps the user could have found similar information\nthrough effective use of a non-generative-AI system, like a traditional search engine indexed over the public\nInternet, so it is questionable that using a generative-AI system was sufficiently \u201csubstantial\u201d to make the\ntask easier to execute. We set this question aside. It is not in scope for us to make claims about whether\nor not existing generative-AI models and systems meet the bar of what, for example, the U.S. executive order\nconsiders a meaningful safety risk [ 130]. Regardless, we can still address the extent to which unlearning\ncan or cannot address cases like this one.\n20\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nYesNo\nMaybeNoRemovalNecessary?\nSu\ufb03cient?Suppressione.g., CSAM, NCII, other strictly forbidden observed informatione.g., personal data that can be processed in certain jurisdictions but not others\ne.g., synthetic CSAM, NCII deepfakes (producible from latent information + user prompts)judges, policymakers will need to make case- or domain-based decisions  about what is reasonableMaybeNoNecessary?\nSu\ufb03cient?e.g., unsafe downstream uses of otherwise innocuous or legitimate outputs judges, policymakers will need to make case- or domain-based decisions about what is reasonable suppression necessary,  see right sidee.g., synthetic CSAM, NCII deepfakes, outputs that resemble in-copyright \u201cSpiderman\u201d or real personal data (producible from latent information + user prompts)e.g., cases where the main issue is consent over use of personal data for training (for which possible model outputs might not be relevant)YesNo\nFigure 3: Following from the prior sections, four simple questions help clarify the usefulness\nof unlearning methods for removal and suppression to address policy aims for Generative AI.\nWe consider if information removal of observed information is necessary and sufficient ( left),\nand similarly if output suppression is necessary and sufficient ( right ). We provide examples of\npotential law and policy areas that could exhibit different answers to these questions. There are\ncases where removal may be necessary, but it is likely that removal is on its own insufficient. To\nmoderate or constrain model outputs, suppression is likely always necessary, but suppression\nmethods will also likely always be imperfect to catch all undesirable outputs.\nFor an additional example, consider a generative-AI system that includes a model trained for\nmolecular generation\u2014for suggesting formulas for new drugs and other molecules. Arguably, one\nof the purposes of such a system is to lower the barrier of expertise required for drug discovery.\nEven so, currently, a generative-AI system cannot on its own definitively determine that the\nmolecules it produces are safe for human consumption; this is the point of lab experiments\nand drug trials [e.g., 128]. Once again, safety in this case is not an isolated property of the\ngenerative-AI model or system. Additional knowledge\u2014in this case, derived through biochemical\nexperimentation and human trials\u2014is often needed to determine if the generative-AI outputs are\nbeneficial or harmful. As discussed with respect to Mismatch 4, the types of control that methods\nfor machine unlearning provide are incapable of preventing such downstream harmful outcomes.\nUnlearning can perhaps limit the information in the model or suppress model outputs, such that\ncertain types of unsafe outputs are less likely, but it cannot guarantee that people will not put\nmodel outputs to unsafe uses [70].\n7 Discussion and Conclusion\nWe have covered a lot of ground, but our main takeaway is fairly simple: there are significant mis-\nmatches between what technical methods for machine unlearning can achieve and aspirations for\nhow these methods could make generative-AI models and systems operationalize law and policy\naims in practice. To close, we briefly summarize key points about these mismatches (Section 7.1)\nand then we offer some concrete takeaways for ML research and AI policy (Section 7.2).\n7.1 Recap: Machine unlearning doesn\u2019t do what you think\nNow, having arrived at the end, we are able to revisit our main arguments through a set of relatively\nsimple questions. With an understanding of the different goals (Section 2), targets (Section 3),\nmethods (Section 4), and potential application domains (Section 6) for machine unlearning, we\ncan ask whether (1) removing certain observed information or (2) suppressing certain outputs is\n(a) necessary or (b) sufficient to meaningfully comply with policy aims. (See Figure 3.)\n21\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nIs observed-information removal necessary? Early goals for machine unlearning involved de-\nveloping efficient methods to remove the effects of observed information (Section 3, Definition 1)\nfrom trained models (Section 2.2). This problem presents interesting and challenging technical\nproblems for ML research, and also finds broader motivations in a particular (and contested)\nintepretation of data-deletion requirements in privacy legislation\u2014namely, the \u201cright to be forgot-\nten\u201d in the GDPR [ 102] (Section 2.1). Setting aside the case of the GDPR, removal from a model\u2019s\ntraining dataset (Section 4.1) may be necessary for some types of information, such as CSAM or\nNCII, where it is illegal or otherwise forbidden to observe this information in the model-training\nprocess (Section 6.3). In such cases, structural removal (Section 4.1) may be necessary and output-\nsuppression methods (Section 4.2), which do not guarantee that information is removed from the\nmodel\u2019s parameters, may not suffice (Section 5, Mismatch 1).\nIn other cases, despite the intuitive alignment of the meanings of the words \u201cremoval\u201d and \u201cdele-\ntion,\u201d it is unclear if technical removal is indeed necessary to satisfy deletion requirements in law\nand policy. This lack of clarity is visible even in simpler cases that do not involve ML models. For in-\nstance, in some circumstances, the CJEU has ruled that suppressing a right-exercising individual\u2019s\npersonal data in certain jurisdictions, rather than wholesale deletion across all jurisdictions, satis-\nfies Article 17 of the GDPR (Section 6.1). While this example does not directly concern Generative\nAI, it is similarly unclear if removal is generally necessary to achieve desired ends in this setting.\nIs observed-information removal sufficient? In limited cases, removal may be sufficient, for\nexample, to satisfy data deletion requests (Section 6.1). In others, even if we could perfectly\nremove the effects of piece of targeted observed information from a model, this would likely not\nbe enough to meet the prescribed goals of machine unlearning for Generative AI (Section 2.3).\nInstead, it may often be more important that a generative-AI model or system is unable to produce\noutputs that reflect certain observed information, certain latent information derived from it, or\ncertain knowledge or abilities (Section 3). If the main goal is to moderate or constrain model\noutputs, removing a targeted piece of problematic observed information does not guarantee that\na generative-AI model could not produce generations that reflect this information at generation\ntime. For example, relying only on the \u201cgold standard\u201d approach to unlearning (Section 4.1) to\nremove real NCII from a model\u2019s training data cannot guarantee that the model could never be\nused to produce NCII deepfakes based on the combination of latent information in the model\n(Section 3, Definition 2) and the user\u2019s prompt (Section 5, Mismatch 3).\nIs output suppression necessary? If preventing outputs like these is the main point (Sections 2.3\n& 2.4), then output suppression is necessary. It is likely more important\u2014both for technical\nmethods (Section 4) and policy objectives (Section 6)\u2014to devote attention to suppressing targeted\ntypes of model outputs. Output suppression, however, is a fundamentally different technical\ngoal from removal of information from a model\u2019s training data (Section 5, Mismatches 2 & 3). To a\ncertain extent, machine-unlearning research in Generative AI acknowledges these differences; one\ncan see this in the shift to expand the family of methods for machine unlearning\u2014to go beyond\nremoval and to include technical approaches for output suppression (Section 4.2). (Indeed, if\nwe attempt to draw lessons from cases like the CJEU example above, courts also clearly appreciate\nthe conceptual differences between removal and suppression in rulings that find that there are\ncases in which suppression is a more appropriate operationalization of compliance with policy.)\nIs output suppression sufficient? While there are cases where output suppression is important, it\nwill not always on its own be sufficient. The appropriateness of some outputs will depend almost\nentirely on the end user and the context of use, not the model or system in isolation (Section 5,\nMismatch 4). Seemingly innocuous or otherwise legitimate outputs could be put to all sorts of\nunsafe downstream uses (Section 6.3), and there is no feasible way for an output-suppression\nmethod like RLHF or an output filter (which is effectively a classifier) to anticipate this type\nof downstream outcome. More generally, output-suppression methods will likely always be\nimperfect (Section 4.2). There will likely be cases where generations violate a policy\u2014e.g., contain\ninformation that resembles real personal data\u2014but are not caught by an output filter and are\nsurfaced to end users. This indicates that the sufficiency of a particular suppression method\nto prevent the generation of undesirable content will depend on the context of the particular\nsystem in which it is applied. For different contexts, policymakers and judges will need to identify\nappropriate guidance for what constitutes success for suppression, system developers will need to\n22\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nfigure out solutions for coming into compliance, and policymakers and judges will need to set\nreasonable expectations concerning whether a system developer has taken reasonable efforts to\nachieve compliance (Section 7.2).45\nThe main takeaway from asking these four questions is that the answers will depend on the\nspecific context in law and policy (Figure 3). Removal may be necessary on occasion for specific\ntypes of data that models should never have seen during training. But removal often is not\nthe main concern for the policy goals where machine unlearning gets invoked as a possible\ntechnical solution for Generative AI. Output suppression may generally be a more appropriate,\nbut nevertheless imperfect, approach. In some cases, both removal and suppression may both\nbe in order. In others still, there can be irreconcilable tensions between removal and suppression.\nThat is, to effectively filter out some generations from being presented to end users, it may in fact\nbe necessary for the overarching generative-AI system to retain and leverage related information\n(Section 4). Even if we were to remove \u201cSpiderman\u201d from a generative-AI model, the generative-AI\nsystem that contains this model would likely still need to have access to information about\n\u201cSpiderman\u201d in order to effectively suppress generations that resemble \u201cSpiderman\u201d (Section 6.2).\n7.2 Takeaways for ML research and AI policy\nFollowing from above, we offer five takeaways for ML researchers and AI policymakers.\nUnlearning is just one approach in the ML and policy toolkit. There are clear gaps for what\nmachine unlearning can do to achieve policy aims, both with respect to methods for removal of\nobserved information and output suppression. Different methods may be useful to certain extents\nin specific contexts, but it is important to view unlearning as just one approach among may others\n(e.g., acceptable usage policies and responsible AI licenses [ 72,92]) that could sometimes help\nachieve specific policy aims. Nevertheless, ML researchers should not claim\u2014and policymakers\nshould not misunderstand\u2014that machine unlearning is generally on its own effective for making\ngenerative-AI models and their outputs compliant with any desired policy goals.\nEvaluation of an unlearning method for a specific domain is a specific task. Further, such general\nclaims about the broader impacts of unlearning are likely to be wrong from first principles because\neach legal and policy regime has its own specific expectations, which can be subtle and nuanced.\nTo make rigorous claims about the broader usefulness of particular unlearning methods, as much\nas possible, ML experts need to evaluate specific unlearning techniques against specific regimes.\nThis requires an understanding of these specific regimes, not just generalized ideas of how they\nmight work\u2014generalized ideas that may be so oversimplified that they are misleading or incorrect.\nTo make claims about how an unlearning method might or might not be useful for operationalizing\ncompliance with Article 17 of the GDPR, a layperson\u2019s reading of the text is not enough. It is impor-\ntant to be familiar with the complexity of different interpretations, rulings, and exceptions. To make\nclaims about the relevance of an unlearning method for U.S. copyright compliance, it is important\nto make specific claims about specific areas of copyright law, rather than to treat copyright law\nas a monolith [77]. At a minimum, this requires understanding those specific areas of copyright.\nThe appropriateness of a particular technical mitigation hinges on these specifics. They cannot\nbe overlooked or abstracted away. For one, as we have seen throughout, these specifics can\nilluminate whether removal or suppression is the right technical goal to pursue for a specific\nsubstantive end. In doing so, it becomes unclear if the original goal of removal of information\nfrom a model (Sections 2.1 & 2.2) is the most relevant technical end to pursue for law and policy\nimpact. In many cases, it seems like output suppression is what interested parties really care\nabout (Sections 2.3 & 2.4). Output suppression\u2013which does not necessarily have anything to\ndo with \u201cunlearning\u201d information from a model\u2019s parameters\u2014is perhaps a more relevant area\nof focus for ML research that aspires to influence policy. For another, a clear understanding\nof the specific goals of specific pieces of law or policy is important for guiding the right set of\nsolutions\u2014technical or otherwise. In some regimes, perfect guarantees may be an unnecessary\nor undue burden for model developers and custodians. It may not be relevant to focus research\nefforts on producing methods that guarantee with certainty that a particular piece of information\n45Both are non-trivial tasks, given the non-determinism of generative-AI system outputs. See Cooper and\nGrimmelmann [26, Part III.D], Wallach et al. [136], and Section 4.2.\n23\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nis removed or suppressed. Reasonable efforts to remove or suppress may be sufficient in some\nlegal contexts, even if their results are imperfect. Of course, this will depend on the needs judges\nand policymakers articulate for the particular domain.\nUnderstanding unlearning as a generative-AI systems problem. From our discussion, it should\nbe clear that machine unlearning for Generative AI does not only concern generative-AI models ; it\nmore generally concerns the generative-AI systems in which these models are embedded. Systems-\nlevel interventions (e.g., content filters) are an important tool for constraining outputs (Section 4.2);\nevaluating such interventions clearly requires systems-level analysis [ 28,104]. Open-weight mod-\nels, like Meta\u2019s family of Llama models [ 87], therefore present different challenges for unlearning.\nThese models are released as their parameters; on their own, they cannot implement system-level\nguardrails\u2014for unlearning or other purposes. In order to achieve this type of functionality, devel-\nopers who use open-weight models for their own systems would need to implement their own\nmechanisms for output suppression, or to incorporate other available software that is intended for\nthis purpose [e.g., 65].\nSetting reasonable goals and expectations for unlearning. It is also important for judges and\npolicymakers to realize that, in general, it is unlikely that technical solutions for unlearning will\nget significantly better anytime soon. It is unlikely that all that is needed is a few more years\nof research and development for unlearning methods to wholly achieve desired policy goals.\nInstead, it will be important to modify expectations for machine unlearning in policy norms. This\nnecessarily includes thinking through specific policy goals and, when using technical methods\nto achieve those goals, what should constitute reasonable best efforts in different contexts with\nrespect to removing or suppressing unwanted information from models and system outputs. For\nexample, judges or regulators may expect best efforts to have observed information removed\nfrom a model and related information suppressed from its outputs; and, if a company meets this\nbar, they would not seek massive fines if somehow the model\u2019s outputs still approximate that\ninformation. We expect that the focus would then become on the remediation process\u2014i.e., did\na developer take reasonable steps\u2014and not perfect results.\nThere are no general-purpose solutions to constrain generative technologies. Finally, and more\ngenerally, policymakers should resist the tendency to think that unlearning methods can lead\nto generative-AI models that can do \u201ceverything but X.\u201d One of the strongest appeals of many\ngenerative-AI systems is that they are general-purpose: they can be adapted to a wide range of uses\nand produce a wide range of useful outputs. A superficial understanding of machine unlearning is\nthat it can surgically and completely remove specific capabilities from a model while leaving every-\nthing else about the model unchanged. As we have seen, this is not what unlearning methods actu-\nally accomplish. The same power to abstract and generalize that makes these models so useful also\nmeans that, with small targeted changes, they are often still capable of exhibiting similar behavior.\nTo use a biological analogy, people who have forgotten a fact will often remember that same fact\nlater when prompted differently to recall it; people who have suffered a stroke can sometimes re-\ngain main of the cognitive functions that were temporarily impaired. The brain is too complex and\ntoo capable for targeted unlearning to be workable; the Men in Black neuralyzer is science fiction.\nThis lesson is familiar from other generative technologies like the PC and the Internet [ 31,151]. Ed\nFelten calls it the \u201cFallacy of the Almost-General-Purpose Computer\u201d [ 49]. For example, the PC\nhas the ability to be adapted to a wide range of computational tasks through suitable configuration\nand inputs; this means that there is no simple or reliable way to prevent a computer (let alone\na generative-AI system) from ever being used to violate privacy, infringe copyright, or design a\ndangerous molecule\u2014not without fundamentally compromising the flexibility and power that\nmake it so useful. A toaster cannot design a bioweapon, but a toaster also cannot do much besides\nmake toast. A computer can, and so can a generative-AI system. This is an inherent tension with\nall generative systems. We can try to tether or constrain different aspects of this generativity in\ndifferent ways, but we will not be able block its capacity for harmful uses with one, comprehensive\nmethod\u2014from either technology or policy\u2014that will work in all possible contexts.\n24\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nAcknowledgments\nWe thank the individual co-organizers of Evaluating Generative AI Systems: the Good, the\nBad, and the Hype (GenLaw DC) for early conversations about machine unlearning that helped\nspark this collaboration. We also thank The K&L Gates Initiative in Ethics and Computational\nTechnologies at CMU, The GenLaw Center, Georgetown Institute for Technology Law & Policy,\nand the Center for Democracy & Technology, who co-hosted the GenLaw DC workshop. We\nthank the reviewers and attendees of the 2nd Workshop on Generative AI + Law atICML \u201924\nfor their feedback on earlier versions of this work. Lastly, we thank Jared Bomberg, Nicholas\nCarlini, Alexandra Givens, Milad Nasr, Adam Roberts, Pam Samuelson, and Jon Small for useful\ndiscussion and feedback on this piece.\nReferences\n[1]Alessandro Achille, Michael Kearns, Carson Klingenberg, and Stefano Soatto. AI Model\nDisgorgement: Methods and Choices, 2023. URL https://arxiv.org/abs/2304.03545 .\n[2]Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan\nMan \u00b4e. Concrete Problems in AI Safety, 2016. URL https://arxiv.org/abs/1606.06565 .\n[3]Anthropic. The case for targeted regulation, October 2024. URL https://www.anthropic.\ncom/news/the-case-for-targeted-regulation .\n[4] Anthropic. Meet Claude, 2024. URL https://www.anthropic.com/claude .\n[5] Art Attacks Ink, LLC v. MGA Ent. Inc. 581 F.3d 1138, 1143 (9th Cir. 2009).\n[6] Authors Guild v. Google, Inc. 804 F.3d 202 (2d Cir. 2015).\n[7] Solon Barocas and Andrew D. Selbst. Big Data\u2019s Disparate Impact, 2014.\n[8]Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James\nGlass. Identifying and Controlling Important Neurons in Neural Machine Translation, 2018.\nURL https://arxiv.org/abs/1811.01157 .\n[9]Yoshua Bengio et al. International Scientific Report on the Safety of Advanced\nAI: Interim Report, May 2024. URL https://assets.publishing.service.gov.uk/\nmedia/66f5311f080bdf716392e922/international scientific report onthesafety of\nadvanced aiinterim report.pdf . AI Seoul Summit.\n[10] Jaydeep Borkar. What can we learn from Data Leakage and Unlearning for Law?, 2023. URL\nhttps://arxiv.org/abs/2307.10476 .\n[11] Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin\nTravers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine Unlearning. In OAKLAND\n\u201921, May 2021. URL https://arxiv.org/pdf/1912.03817.pdf .\n[12] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian\nTram `er. What Does it Mean for a Language Model to Preserve Privacy? In Proceedings of the\n2022 ACM Conference on Fairness, Accountability, and Transparency , FAccT \u201922, page 2280\u20132292,\nNew York, NY, USA, 2022. Association for Computing Machinery. ISBN 9781450393522.\nURL https://doi.org/10.1145/3531146.3534642 .\n[13] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel,\nAllan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff,\nGregory C. Allen, Jacob Steinhardt, Carrick Flynn, Se \u00b4an\u00b4O h \u00b4Eigeartaigh, Simon Beard,\nHaydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael\nPage, Joanna Bryson, Roman Yampolskiy, and Dario Amodei. The Malicious Use of Artificial\nIntelligence: Forecasting, Prevention, and Mitigation, 2018. URL https://arxiv.org/abs/\n1802.07228 .\n[14] California State Legislature. California consumer privacy act of 2018 (ccpa), 2018. URL\nhttps://cppa.ca.gov/regulations/pdf/cppa act.pdf .\n25\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[15] California State Legislature. SB 1047: Safe and Secure Innovation for Frontier Artificial Intelli-\ngence Models Act. |Digital Democracy, 2024. URL https://digitaldemocracy.calmatters.\norg/bills/ca 202320240sb1047 .\n[16] Campbell v. Acuff-Rose Music, Inc. 510 U.S. 569 (1994).\n[17] Yinzhi Cao and Junfeng Yang. Towards Making Systems Forget with Machine Unlearning. In\n2015 IEEE Symposium on Security and Privacy , pages 463\u2013480, 2015. doi: 10.1109/SP .2015.35.\n[18] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram `er, and\nChiyuan Zhang. Quantifying Memorization Across Neural Language Models. In Interna-\ntional Conference on Learning Representations , 2023.\n[19] Gert Cauwenberghs and Tomaso Poggio. Incremental and Decremental Support Vector\nMachine Learning. Adv. Neural Inf. Process. Syst. , 1, February 2001.\n[20] Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms,\n2023. URL https://arxiv.org/abs/2310.20150 .\n[21] Alexandra Chouldechova, A. Feder Cooper, Abhinav Palia, Dan Vann, Chad Atalla, Hannah\nWashington, Emily Sheng, and Hanna Wallach. Red Teaming: Everything Everywhere All\nat Once. In Neurips Safe Generative AI Workshop 2024 , 2024. URL https://openreview.net/\nforum?id=KEggQCeDUA .\n[22] Somnath Basu Roy Chowdhury, Krzysztof Choromanski, Arijit Sehanobish, Avinava Dubey,\nand Snigdha Chaturvedi. Towards scalable exact machine unlearning using parameter-\nefficient fine-tuning, 2024. URL https://arxiv.org/abs/2406.16257 .\n[23] Vikram S. Chundawat, Ayush K. Tarun, Murari Mandal, and Mohan Kankanhalli. Zero-Shot\nMachine Unlearning. Trans. Info. For. Sec. , 18:2345\u20132354, January 2023. ISSN 1556-6013. URL\nhttps://doi.org/10.1109/TIFS.2023.3265506 .\n[24] Aloni Cohen, Adam Smith, Marika Swanberg, and Prashant Nalini Vasudevan. Control, Con-\nfidentiality, and the Right to be Forgotten, 2023. URL https://arxiv.org/abs/2210.07876 .\n[25] Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri `a\nGarriga-Alonso. Towards Automated Circuit Discovery for Mechanistic Interpretability,\n2023. URL https://arxiv.org/abs/2304.14997 .\n[26] A. Feder Cooper and James Grimmelmann. The Files are in the Computer: Copyright,\nMemorization, and Generative-AI Systems. arXiv preprint arXiv:2404.12590 , 2024.\n[27] A. Feder Cooper and Karen Levy. Fast or Accurate? Governing Conflicting Goals in Highly\nAutonomous Vehicles. Colorado Technology Law Journal , 20:249\u2013277, 2022.\n[28] A. Feder Cooper, Karen Levy, and Christopher De Sa. Accuracy-Efficiency Trade-Offs and\nAccountability in Distributed ML Systems. In Equity and Access in Algorithms, Mechanisms,\nand Optimization , EAAMO \u201921, New York, NY, USA, 2021. Association for Computing\nMachinery. ISBN 9781450385534. doi: 10.1145/3465416.3483289.\n[29] A. Feder Cooper, Emanuel Moss, Benjamin Laufer, and Helen Nissenbaum. Accountability\nin an Algorithmic Society: Relationality, Responsibility, and Robustness in Machine Learn-\ning. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency ,\nFAccT \u201922, page 864\u2013876, New York, NY, USA, 2022. Association for Computing Machinery.\nISBN 9781450393522. doi: 10.1145/3531146.3533150.\n[30] A. Feder Cooper, Wentao Guo, Khiem Pham, Tiancheng Yuan, Charlie F. Ruan, Yucheng Lu,\nand Christopher De Sa. Coordinating Distributed Example Orders for Provably Accelerated\nTraining. In Thirty-seventh Conference on Neural Information Processing Systems , 2023. URL\nhttps://openreview.net/forum?id=ISRyILhAyS .\n26\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[31] A. Feder Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher\nCallison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage,\nDavid Mimno, Madiha Zahrah Choksi, Jack M. Balkin, Nicholas Carlini, Christopher De\nSa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, Andres Guadamuz, Swee Leng Harris,\nAbigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, Mark Lemley, Cass Matthews, Christine\nMcLeavey, Corynne McSherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela\nSamuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, and Elana Zeide. Report\nof the 1st Workshop on Generative AI and Law. arXiv preprint arXiv:2311.06477 , 2023.\n[32] A. Feder Cooper, Katherine Lee, Madiha Zahrah Choksi, Solon Barocas, Christopher De Sa,\nJames Grimmelmann, Jon Kleinberg, Siddhartha Sen, and Baobao Zhang. Arbitrariness\nand Social Prediction: The Confounding Role of Variance in Fair Classification. Proceedings\nof the AAAI Conference on Artificial Intelligence , 38(20):22004\u201322012, March 2024.\n[33] Copyright Law of the United States. 17 U.S. Code \u00a7102 - Subject matter of copyright: In\ngeneral, December 1990. URL https://www.law.cornell.edu/uscode/text/17/102 .\n[34] Copyright Law of the United States. 17 U.S. Code \u00a7107 - Limitations on exclusive rights:\nFair use, October 1992. URL https://www.law.cornell.edu/uscode/text/17/107 .\n[35] Copyright Law of the United States. 17 U.S. Code \u00a7506 - Criminal offenses, October 2008.\nURL https://www.law.cornell.edu/uscode/text/17/506 .\n[36] Copyright Law of the United States. 17 U.S. Code \u00a7503 - Remedies for infringe-\nment: Impounding and disposition of infringing articles, December 2010. URL\nhttps://www.law.cornell.edu/uscode/text/17/503 .\n[37] Copyright Law of the United States. 17 U.S. Code \u00a7504 - Remedies for infringement: Dam-\nages and profits, December 2010. URL https://www.law.cornell.edu/uscode/text/17/504 .\n[38] Copyright Law of the United States. 17 U.S. Code \u00a7512 - Limitations on liability relating to\nmaterial online, December 2010. URL https://www.law.cornell.edu/uscode/text/17/512 .\n[39] Court of Justice of the European Union. Judgment in Case C-507/17 Google\nLLC, successor in law to Google Inc. v Commission nationale de l\u2019informatique\net des libert \u00b4es (CNIL), September 2019. URL https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX%3A62017CJ0507 . Press Release No. 112/19, Luxembourg.\n[40] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge\nNeurons in Pretrained Transformers. In Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages 8493\u20138502, Dublin, Ireland, May 2022.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.581. URL\nhttps://aclanthology.org/2022.acl-long.581 .\n[41] Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah\nSmith. Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and\nEarly Stopping, 2020. URL https://arxiv.org/abs/2002.06305 .\n[42] Thorsten Eisenhofer, Doreen Riepel, Varun Chandrasekaran, Esha Ghosh, Olga Ohrimenko,\nand Nicolas Papernot. Verifiable and Provably Secure Machine Unlearning, 2023.\n[43] Ronen Eldan and Mark Russinovich. Who\u2019s Harry Potter? Approximate Unlearning in\nLLMs, 2023.\n[44] European Union. Regulation (EU) 2024/1689 of the European Parliament and of the Council\nof 13 June 2024 laying down harmonised rules on artificial intelligence and amending\nRegulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858,\n(EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)\n2020/1828 (Artificial Intelligence Act), 2024. URL https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/?uri=CELEX:32024R1689 . Official Journal of the European Union.\n27\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[45] Federico Fabbrini and Edoardo Celeste. The Right to Be Forgotten in the Digital Age: The\nChallenges of Data Protection Beyond Borders. German Law Journal , 21(S1):55\u201365, 2020. doi:\n10.1017/glj.2020.14.\n[46] Michael Feffer, Anusha Sinha, Wesley Hanwen Deng, Zachary C. Lipton, and Hoda\nHeidari. Red-Teaming for Generative AI: Silver Bullet or Security Theater?, 2024. URL\nhttps://arxiv.org/abs/2401.15897 .\n[47] Feist Publications v. Rural Telephone Service Company. 499 U.S. 340 (1991).\n[48] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In\nProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , STOC 2020,\npage 954\u2013959, New York, NY, USA, 2020. Association for Computing Machinery. ISBN\n9781450369794.\n[49] Ed Felten. The Fallacy of the Almost-General-Purpose Computer, October 2002. URL https:\n//freedom-to-tinker.com/2002/10/14/fallacy-almost-general-purpose-computer/ .\n[50] Luciano Floridi. Machine Unlearning: Its Nature, Scope, and Importance for a \u201dDelete\nCulture\u201d. Philosophy & Technology , 36, 2023.\n[51] Geneva Internet Platform. AI\u2019s right to forget \u2013 Machine unlearning. digwatch , August 2023.\nURL https://dig.watch/updates/ais-right-to-forget-machine-unlearning .\n[52] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer Feed-Forward\nLayers Are Key-Value Memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,\nand Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing , pages 5484\u20135495, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-\nmain.446. URL https://aclanthology.org/2021.emnlp-main.446 .\n[53] Antonio A. Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. Making AI forget\nyou: data deletion in machine learning. In Proceedings of the 33rd International Conference\non Neural Information Processing Systems , Red Hook, NY, USA, 2019. Curran Associates Inc.\n[54] Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, and Amartya\nSanyal. Corrective Machine Unlearning, 2024. URL https://arxiv.org/abs/2402.14015 .\n[55] Aaron Gokaslan, A. Feder Cooper, Jasmine Collins, Landan Seguin, Austin Jacobson, Mihir\nPatel, Jonathan Frankle, Cory Stephenson, and Volodymyr Kuleshov. CommonCanvas:\nAn Open Diffusion Model Trained with Creative-Commons Images. arXiv preprint\narXiv:2310.16825 , 2023.\n[56] Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An Empirical\nInvestigation of Catastrophic Forgetting in Gradient-Based Neural Networks, 2015.\n[57] James Grimmelmann. Copyright for Literate Robots. Iowa Law Review , 101:657, 2016.\n[58] Chuan Guo, Tom Goldstein, Awni Y. Hannun, and Laurens van der Maaten. Certi-\nfied data removal from machine learning models. CoRR , abs/1911.03030, 2019. URL\nhttp://arxiv.org/abs/1911.03030 .\n[59] Sarah Hastings-Woodhouse. Introduction to Mechanistic Interpretability, 2024. URL https:\n//aisafetyfundamentals.com/blog/introduction-to-mechanistic-interpretability/ .\n[60] Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A.\nLemley, and Percy Liang. Foundation models and fair use, 2023. URL\nhttps://arxiv.org/abs/2303.15715 .\n[61] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved Problems\nin ML Safety, 2022. URL https://arxiv.org/abs/2109.13916 .\n[62] Alex Hern. Facebook launching tools to tackle revenge porn. The Guardian , April 2017. URL\nhttps://www.theguardian.com/technology/2017/apr/05/facebook-tools-revenge-porn .\n28\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[63] Emmie Hine, Claudio Novelli, Mariarosaria Taddeo, and Luciano Floridi. Supporting Trust-\nworthy AI Through Machine Unlearning. Science and Engineering Ethics , 30, September 2024.\n[64] Yangsibo Huang, Daogao Liu, Lynn Chua, Badih Ghazi, Pritish Kamath, Ravi Ku-\nmar, Pasin Manurangsi, Milad Nasr, Amer Sinha, and Chiyuan Zhang. Unlearn and\nBurn: Adversarial Machine Unlearning Requests Destroy Model Accuracy, 2024. URL\nhttps://arxiv.org/abs/2410.09591 .\n[65] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao,\nMichael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. Llama\nGuard: LLM-based Input-Output Safeguard for Human-AI Conversations, 2023. URL\nhttps://arxiv.org/abs/2312.06674 .\n[66] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran,\nand Minjoon Seo. Knowledge Unlearning for Mitigating Privacy Risks in Language Models,\n2022. URL https://arxiv.org/abs/2210.01504 .\n[67] Hyejun Jeong, Shiqing Ma, and Amir Houmansadr. SoK: Challenges and Opportunities\nin Federated Unlearning, 2024.\n[68] Susmit Jha and Sanjit A Seshia. A theory of formal synthesis via inductive learning. Acta\nInformatica , 54:693\u2013726, 2017.\n[69] Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer,\nBhavya Kailkhura, and Sijia Liu. Soul: Unlocking the power of second-order optimization\nfor llm unlearning, 2024. URL https://arxiv.org/abs/2404.18239 .\n[70] Erik Jones, Anca Dragan, and Jacob Steinhardt. Adversaries Can Misuse Combinations\nof Safe Models, 2024. URL https://arxiv.org/abs/2406.14595 .\n[71] Bj\u00f8rn Aslak Juliussen, Jon Petter Rui, and Dag Johansen. Algorithms that forget:\nMachine unlearning and the right to erasure. Computer Law & Security Review , 51:\n105885, 2023. ISSN 0267-3649. doi: https://doi.org/10.1016/j.clsr.2023.105885. URL\nhttps://www.sciencedirect.com/science/article/pii/S026736492300095X .\n[72] Kevin Klyman. Acceptable Use Policies for Foundation Models. arXiv preprint\narXiv:2409.09041 , 2024.\n[73] Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, and Dan Roth. Privacy Adhering\nMachine Un-learning in NLP. In Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya,\nAyu Purwarianti, and Adila Alfa Krisnadhi, editors, Findings of the Association for Computa-\ntional Linguistics: IJCNLP-AACL 2023 (Findings) , pages 268\u2013277, Nusa Dua, Bali, November\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-ijcnlp.25.\nURL https://aclanthology.org/2023.findings-ijcnlp.25 .\n[74] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards\nUnbounded Machine Unlearning, 2023. URL https://arxiv.org/abs/2302.09880 .\n[75] Rachel Layne. How to Make AI \u2018Forget\u2019 All the Private Data It Shouldn\u2019t Have. Harvard\nBusiness School , February 2024. URL https://hbswk.hbs.edu/item/qa-seth-neel-on-\nmachine-unlearning-and-the-right-to-be-forgotten .\n[76] Katherine Lee, A. Feder Cooper, James Grimmelmann, and Daphne Ippolito. AI and Law:\nThe Next Generation, 2023.\n[77] Katherine Lee, A. Feder Cooper, and James Grimmelmann. Talkin\u2019 \u2019Bout AI Generation:\nCopyright and the Generative-AI Supply Chain. arXiv preprint arXiv:2309.08133 , 2023.\n[78] Mark Lemley and Bryan Casey. Fair Learning. Texas Law Review , 99:743, 2021.\n[79] Pierre N. Leval. Toward a Fair Use Standard. Harvard Law Review , 103(5):1105, 1990.\n[80] Guihong Li, Hsiang Hsu, Chun-Fu Chen, and Radu Marculescu. Machine Unlearning for\nImage-to-Image Generative Models, 2024. URL https://arxiv.org/abs/2402.00351 .\n29\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[81] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D.\nLi, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-\nBurger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass,\nOliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao,\nAriel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika,\nZifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih,\nKemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis,\nAlex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen\nFitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu\nWang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexander Wang, and Dan Hendrycks.\nThe WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning, 2024.\n[82] Ken Ziyu Liu. Machine Unlearning in 2024, May 2024. URL https://ai.stanford.edu/\n\u223ckzliu/blog/unlearning .\n[83] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase,\nXiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney, Mohit Bansal, Sanmi Koyejo, and\nYang Liu. Rethinking Machine Unlearning for Large Language Models, 2024.\n[84] Yang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip Yu,\nand Kui Ren. Learn to Forget: Machine Unlearning via Neuron Masking, 2021. URL\nhttps://arxiv.org/abs/2003.10933 .\n[85] Yi Liu, Lei Xu, Xingliang Yuan, Cong Wang, and Bo Li. The Right to be For-\ngotten in Federated Learning: An Efficient Realization with Rapid Retraining.\nInIEEE INFOCOM 2022 - IEEE Conference on Computer Communications , page\n1749\u20131758. IEEE Press, 2022. doi: 10.1109/INFOCOM48880.2022.9796721. URL\nhttps://doi.org/10.1109/INFOCOM48880.2022.9796721 .\n[86] Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. To-\nwards Safer Large Language Models through Machine Unlearning, 2024. URL\nhttps://arxiv.org/abs/2402.10058 .\n[87] AI Meta Llama Team. The Llama 3 Herd of Models, 2024. URL https:\n//ai.meta.com/research/publications/the-llama-3-herd-of-models/ .\n[88] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj\nAmmanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced\nunlearning, 2022. URL https://arxiv.org/abs/2205.13636 .\n[89] Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell.\nEight Methods to Evaluate Robust Unlearning in LLMs, 2024.\n[90] Ananth Mahadevan and Michael Mathioudakis. Certifiable Machine Unlearning for Linear\nModels, 2021. URL https://arxiv.org/abs/2106.15093 .\n[91] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. TOFU:\nA Task of Fictitious Unlearning for LLMs, 2024. URL https://arxiv.org/abs/2401.06121 .\n[92] Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse Josua Benjamin, Jenny Lee, Yacine\nJernite, Carlos Mu \u02dcnoz Ferrandis, Aaron Gokaslan, Alek Tarkowski, Joseph Lindley, A. Feder\nCooper, and Danish Contractor. On the standardization of behavioral use clauses and their\nadoption for responsible licensing of ai. arXiv preprint arXiv:2402.05979 , 2024.\n[93] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and Editing\nFactual Associations in GPT. Advances in Neural Information Processing Systems , 36, 2022.\narXiv:2202.05262.\n[94] Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza\nShokri, and Yejin Choi. Can LLMs Keep a Secret? Testing Privacy Implications of Language\nModels via Contextual Integrity Theory, 2024. URL https://arxiv.org/abs/2310.17884 .\n30\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[95] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea\nFinn. Memory-Based Model Editing at Scale. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceed-\nings of the 39th International Conference on Machine Learning , volume 162 of Proceed-\nings of Machine Learning Research , pages 15817\u201315831. PMLR, 17\u201323 Jul 2022. URL\nhttps://proceedings.mlr.press/v162/mitchell22a.html .\n[96] Christine Mui. AI\u2019s Next Challenge: how to forget. Politico , June 2024. URL\nhttps://www.politico.com/newsletters/digital-future-daily/2024/06/03/ai-\nmachine-unlearning-forget-00161303 .\n[97] Neel Nanda. A Comprehensive Mechanistic Interpretability Explainer & Glossary, 2023.\nURL https://www.neelnanda.io/mechanistic-interpretability/glossary .\n[98] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt.\nProgress measures for grokking via mechanistic interpretability, 2023. URL\nhttps://arxiv.org/abs/2301.05217 .\n[99] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper,\nDaphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tram `er, and\nKatherine Lee. Scalable Extraction of Training Data from (Production) Language Models.\narXiv preprint arXiv:2311.17035 , 2023.\n[100] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-Delete: Gradient-Based\nMethods for Machine Unlearning, 2020.\n[101] Office of Science and Technology Policy. White House Announces New Private Sector\nVoluntary Commitments to Combat Image-Based Sexual Abuse, September 2024. URL\nhttps://www.whitehouse.gov/ostp/news-updates/2024/09/12/white-house-announces-\nnew-private-sector-voluntary-commitments-to-combat-image-based-sexual-abuse/ .\nThe White House.\n[102] Official Journal of the European Union. Regulation (EU) 2016/679 (General\nData Protection Regulation), April 2016. URL https://eur-lex.europa.eu/legal-\ncontent/EN/TXT/PDF/?uri=CELEX:32016R0679 .\n[103] OpenAI. ChatGPT: Optimizing Language Models for Dialogue, 2022. URL\nhttps://web.archive.org/web/20221130180912/https://openai.com/blog/chatgpt/ .\n[104] OpenAI. GPT-4 System Card, March 2023. URL https://cdn.openai.com/papers/gpt-4-\nsystem-card.pdf .\n[105] OpenAI. Preparedness Framework (Beta), 2023. URL https://cdn.openai.com/openai-\npreparedness-framework-beta.pdf .\n[106] Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-Context Unlearning: Language\nModels as Few Shot Unlearners, 2024. URL https://arxiv.org/abs/2310.07579 .\n[107] Fabio Petroni, Tim Rockt \u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H\nMiller, and Sebastian Riedel. Language Models as Knowledge Bases? arXiv preprint\narXiv:1909.01066 , 2019.\n[108] USVSN Sai Prashanth, Alvin Deng, Kyle O\u2019Brien, Jyothir S V au2, Mohammad Aflah Khan,\nJaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy\nKe, Katherine Lee, and Naomi Saphra. Recite, Reconstruct, Recollect: Memorization in LMs\nas a Multifaceted Phenomenon, 2024. URL https://arxiv.org/abs/2406.17746 .\n[109] Youyang Qu, Xin Yuan, Ming Ding, Wei Ni, Thierry Rakotoarivelo, and David Smith. Learn\nto Unlearn: A Survey on Machine Unlearning, 2023.\n[110] Rentmeester v. Nike, Inc. 883 F.3d 1111 (9th Cir. 2018).\n31\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[111] Maarten Sap, Ronan Le Bras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie,\nHannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. Atomic: An atlas of\nmachine commonsense for if-then reasoning. In Proceedings of the AAAI conference on artificial\nintelligence , volume 33, pages 3027\u20133035, 2019.\n[112] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are Emergent Abilities of Large\nLanguage Models a Mirage? In Thirty-seventh Conference on Neural Information Processing\nSystems , 2023. URL https://openreview.net/forum?id=ITw9edRDlD .\n[113] Selle v. Gibb. 741 F.2d 896 (7th Cir. 1984).\n[114] Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Xiaofeng Zhu, and Qing Li. Exploring\nthe Landscape of Machine Unlearning: A Comprehensive Survey and Taxonomy, 2024.\n[115] Ilia Shumailov, Jamie Hayes, Eleni Triantafillou, Guillermo Ortiz-Jimenez, Nicolas Papernot,\nMatthew Jagielski, Itay Yona, Heidi Howard, and Eugene Bagdasaryan. UnUnlearning:\nUnlearning is not sufficient for content regulation in advanced generative AI, 2024. URL\nhttps://arxiv.org/abs/2407.00106 .\n[116] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and\nYarin Gal. Ai models collapse when trained on recursively generated data. Nature , 631\n(8022):755\u2013759, 2024.\n[117] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang.\nKnowledge Unlearning for LLMs: Tasks, Methods, and Challenges, 2023.\n[118] Alison Snyder. Machine forgetting: How difficult it is to get AI to forget. Axios , January\n2024. URL https://www.axios.com/2024/01/12/ai-forget-unlearn-data-privacy .\n[119] Emily H. Soice, Rafael Rocha, Kimberlee Cordova, Michael Specter, and Kevin M. Esvelt.\nCan large language models democratize access to dual-use biotechnology?, 2023. URL\nhttps://arxiv.org/abs/2306.03809 .\n[120] Daniel J. Solove and Woodrow Hartzog. The Great Scrape: The Clash Between Scraping\nand Privacy, 2024. URL https://scholarship.law.bu.edu/faculty scholarship/3917 .\nWorking Paper.\n[121] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein.\nDiffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models, 2022.\n[122] David Marco Sommer, Liwei Song, Sameer Wagh, and Prateek Mittal. Towards Probabilistic\nVerification of Machine Unlearning, 2020.\n[123] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the Imitation\nGame: Quantifying and extrapolating the capabilities of language models, 2023. URL\nhttps://arxiv.org/abs/2206.04615 .\n[124] Elham Tabassi. Artificial Intelligence Risk Management Framework (AI RMF 1.0). Technical\nReport NIST AI 100-1, National Institute of Standards and Technology (U.S.), Gaithersburg,\nMD, January 2023. URL http://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf .\n[125] Rishub Tamirisa, Bhrugu Bharathi, Long Phan, Andy Zhou, Alice Gatti, Tarun Suresh,\nMaxwell Lin, Justin Wang, Rowan Wang, Ron Arel, Andy Zou, Dawn Song, Bo Li, Dan\nHendrycks, and Mantas Mazeika. Tamper-Resistant Safeguards for Open-Weight LLMs,\n2024. URL https://arxiv.org/abs/2408.00761 .\n[126] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. Understanding the\nCapabilities, Limitations, and Societal Impact of Large Language Models, 2021. URL\nhttps://arxiv.org/abs/2102.02503 .\n[127] Gemini Team et al. Gemini: A Family of Highly Capable Multimodal Models, 2023.\n32\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[128] Thomas C. Terwilliger, Dorothee Liebschner, Tristan I. Croll, Christopher J. Williams, Airlie J.\nMcCoy, Billy K. Poon, Pavel V . Afonine, Robert D. Oeffner, Jane S. Richardson, Randy J. Read,\nand Paul D. Adams. AlphaFold predictions are valuable hypotheses, and accelerate but\ndo not replace experimental structure determination. bioRxiv , 2023. doi: 10.1101/2022.11.21.\n517405. URL https://www.biorxiv.org/content/early/2023/05/19/2022.11.21.517405 .\n[129] Pratiksha Thaker, Yash Maurya, Shengyuan Hu, Zhiwei Steven Wu, and Virginia Smith.\nGuardrail baselines for unlearning in llms, 2024. URL https://arxiv.org/abs/2403.03329 .\n[130] The White House. Executive Order on the Safe, Secure, and Trustworthy Development and\nUse of Artificial Intelligence, October 2023. URL https://www.whitehouse.gov/briefing-\nroom/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-\ntrustworthy-development-and-use-of-artificial-intelligence/ . The White House.\n[131] Eleni Triantafillou, Peter Kairouz, Fabian Pedregosa, Jamie Hayes, Meghdad Kurmanji,\nKairan Zhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan,\nLisheng Sun Hosoya, Sergio Escalera, Gintare Karolina Dziugaite, Peter Triantafillou, and\nIsabelle Guyon. Are we making progress in unlearning? Findings from the first NeurIPS\nunlearning competition, 2024. URL https://arxiv.org/abs/2406.09073 .\n[132] UK Government. Iconic Bletchley Park to host UK AI Safety Summit in early November,\n2023. URL https://www.gov.uk/government/news/iconic-bletchley-park-to-host-uk-\nai-safety-summit-in-early-november . Press release.\n[133] U.S. Department of Commerce. Department of Commerce Announces New Guid-\nance, Tools 270 Days Following President Biden\u2019s Executive Order on AI, July 2024.\nURL https://www.commerce.gov/news/press-releases/2024/07/department-commerce-\nannounces-new-guidance-tools-270-days-following .\n[134] UT News. Machine \u2018Unlearning\u2019 Helps Generative AI \u2018Forget\u2019 Copyright-Protected and\nViolent Content, 2024. URL https://news.utexas.edu/2024/03/21/machine-unlearning-\nhelps-generative-ai-forget-copyright-protected-and-violent-content/ .\n[135] Virginia State Legislature. Virginia consumer data protection act of 2021 (vcdpa), 2021. URL\nhttps://law.lis.virginia.gov/vacodefull/title59.1/chapter53/ .\n[136] Hanna Wallach, Meera Desai, Nicholas Pangakis, A. Feder Cooper, Angelina Wang, Solon\nBarocas, Alexandra Chouldechova, Chad Atalla, Su Lin Blodgett, Emily Corvi, P . Alex\nDow, Jean Garcia-Gathright, Alexandra Olteanu, Stefanie Reed, Emily Sheng, Dan Vann,\nJennifer Wortman Vaughan, Matthew Vogel, Hannah Washington, and Abigail Z. Jacobs.\nEvaluating Generative AI Systems is a Social Science Measurement Challenge. arXiv preprint\narXiv:2411.10939 , 2024.\n[137] Boyi Wei, Weijia Shi, Yangsibo Huang, Noah A. Smith, Chiyuan Zhang, Luke Zettlemoyer,\nKai Li, and Peter Henderson. Evaluating Copyright Takedown Methods for Language\nModels, 2024. URL https://arxiv.org/abs/2406.18664 .\n[138] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large\nLanguage Models, 2022. URL https://arxiv.org/abs/2206.07682 .\n[139] Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks,\nJuan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel,\nVerena Rieser, and William Isaac. Sociotechnical Safety Evaluation of Generative AI Systems,\n2023. URL https://arxiv.org/abs/2310.11986 .\n[140] Kyle Wiggers. Making AI Models Forget Undesirable Data Hurts Their Performance.\nTechCrunch , July 2024. URL https://techcrunch.com/2024/07/29/making-ai-models-\nforget-undesirable-data-hurts-their-performance/ .\n[141] Daniel Wilf-Townsend. The Deletion Remedy. North Carolina Law Review , 103, 2024. URL\nhttps://papers.ssrn.com/sol3/papers.cfm?abstract id=4933011 . Forthcoming 2025.\n33\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\n[142] Boming Xia, Qinghua Lu, Liming Zhu, and Zhenchang Xing. An AI System Evaluation\nFramework for Advancing AI Safety: Terminology, Taxonomy, Lifecycle Mapping, July 2024.\n[143] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine Unlearning:\nA Survey, 2023.\n[144] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin. Arcane: An\nefficient architecture for exact machine unlearning. In Lud De Raedt, editor, Proceedings of the\nThirty-First International Joint Conference on Artificial Intelligence, IJCAI-22 , pages 4006\u20134013.\nInternational Joint Conferences on Artificial Intelligence Organization, 7 2022. doi:\n10.24963/ijcai.2022/556. URL https://doi.org/10.24963/ijcai.2022/556 . Main Track.\n[145] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning, 2024. URL\nhttps://arxiv.org/abs/2310.10683 .\n[146] Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Zewei Shi, Sheng Shen, Wanlei\nZhou, and Minhui Xue. Reinforcement Unlearning, 2024.\n[147] Yi Zeng, Kevin Klyman, Andy Zhou, Yu Yang, Minzhou Pan, Ruoxi Jia, Dawn Song,\nPercy Liang, and Bo Li. AI Risk Categorization Decoded (AIR 2024): From Government\nRegulations to Corporate Policies, 2024. URL https://arxiv.org/abs/2406.17864 .\n[148] D. Zhang, P . Finckenberg-Broman, T. Hoang, et al. Right to be forgotten in the Era of\nlarge language models: implications, challenges, and solutions. AI Ethics , 2024. doi:\n10.1007/s43681-024-00573-9. URL https://doi.org/10.1007/s43681-024-00573-9 .\n[149] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative Preference Op-\ntimization: From Catastrophic Collapse to Effective Unlearning, 2024. URL\nhttps://arxiv.org/abs/2404.05868 .\n[150] Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, and\nSijia Liu. UnlearnCanvas: A Stylized Image Dataset to Benchmark Machine Unlearning\nfor Diffusion Models, 2024.\n[151] Jonathan Zittrain. The Future of the Internet\u2013And How to Stop It . Yale University Press, USA,\n2008. ISBN 0300124872.\n[152] Jakub \u0141ucki, Boyi Wei, Yangsibo Huang, Peter Henderson, Florian Tram `er, and Javier\nRando. An Adversarial Perspective on Machine Unlearning for AI Safety, 2024. URL\nhttps://arxiv.org/abs/2409.18025 .\n34\n Preprint. Prior version presented at the 2nd Workshop on Generative AI + Law atICML \u201924 .\nA Growth of unlearning papers over time\nWe provide some cursory evidence to support that there has been massive growth of machine\nunlearning papers in the last few years (Figure 4). Since we draw our results from arXiv, they\ndo not include mentions of machine unlearning in technical reports (e.g., [ 9]) or other literature\noutside of computer science (e.g., [50]).\n2017 2018 2019 2020 2021 2022 2023 2024\nYear04080120160200240280320360400440Number of papersGPT-2 T5 DALL-EPaLMSDChatGPTClaudeUnlearning papers posted to arXiv over time\nFigure 4: We scrape all papers that match unlearn* ormodel forgetting from arXiv and plot\ntheir counts over time, as of December 4, 2024. As of this date, there were a total of 810 papers\nstarting from 1997 that matched out query. We indicate some important dates in the release\nof contemporary language and image generation models: GPT-2 ,T5,DALL-E ,PaLM ,Stable\nDiffusion (SD) ,ChatGPT , and Claude .\nGDPR was passed in 2016, and went into a effect in 2018. 790 of the papers have posting dates\nstarting in 2016 (i.e., only 20 papers precede 2016). Of these 790 papers, 106 (i.e., 13.1%) mention\n\u201cGDPR,\u201d \u201cthe right to be forgotten,\u201d or \u201cRTBF\u201d in the abstract. (These 106 papers are all from\n2020-2024.) Given that we do not search the contents of all of the papers for these phrases, this\nserves as a lower bound of machine-unlearning papers that reference GDPR.\nWe also manually coded each paper with different categories, which we then used to assist with\nour literature review for the paper. Note that, as of December 4, there have been more unlearning\npapers (428) posted to arXiv in 2024 than there were in all prior years combined. While not easily\nvisible in Figure 4, there were 3 papers in 2017.\n35\n", "summary": "<li><strong>Lessons for Generative AI Policy, Research, and Practice</strong>: Machine Unlearning Doesn\u2019t Do What You Think:\nLessons for Generative AI Policy, Research, and Practice\nA. Feder Cooper\u2217\u22c61,2,3Christopher A. Choquette-Choo\u22174Miranda Bogen\u22175,6\nMatthew Jagielski\u22174Katja Filippova\u22174Ken Ziyu Liu\u22173\nAlexandra Chouldechova2Jamie Hayes4Yangsibo Huang7Niloofar Mireshghallah8\nIlia Shumailov4Eleni Triantafillou4Peter Kairouz7Nicole Mitchell7\nPercy Liang3Daniel E. Ho9Yejin Choi8Sanmi Koyejo3Fernando Delgado10\nJames Grimmelmann1,11,12Vitaly Shmatikov11Christopher De Sa13Solon Barocas2\nAmy Cyphert14Mark Lemley9danah boyd2Jennifer Wortman Vaughan2\n</li>\n<li><strong>Our goal is to provide conceptual clarity that elicits these gaps, and to encourage more thoughtful\ncommunication among ML, law, and policy experts who seek to develop and apply technical</strong>: Our goal is to provide conceptual clarity that elicits these gaps, and to encourage more thoughtful\ncommunication among ML, law, and policy experts who seek to develop and apply technical\nmethods for compliance with law and policy objectives (Sections 6 & 7).</li>\n<li><strong>In general, removal on its own is often neither necessary nor sufficient to constrain model outputs\nin a controlled manner.3Instead, suppressing certain model outputs from being surfaced to users</strong>: In general, removal on its own is often neither necessary nor sufficient to constrain model outputs\nin a controlled manner.3Instead, suppressing certain model outputs from being surfaced to users\nmay be a more appropriate area of focus for technical methods</li>\n<li><strong>ML experts should focus their research and how policymakers can adjust their expectations and</strong>: In light of these limitations, we provide recommendations on how\nML experts should focus their research and how policymakers can adjust their expectations and\nnorms concerning reasonable best efforts when using an unlearning method in practice (Section 7).\n</li>\n<li><strong>In other words, the scope for unlearning no longer just concerns what we, following Cooper and</strong>: In other words, the scope for unlearning no longer just concerns what we, following Cooper and\nGrimmelmann</li>\n<li><strong>Carlos is going to be in Philadelphia on Thursday.14This information is not literally contained</strong>: For\nexample, given the observed information \u201cCarlos is going to Susan\u2019s house for a birthday party\nthis Thursday\u201d and \u201cSusan lives in Philadelphia,\u201d a possible piece of latent information is that\nCarlos is going to be in Philadelphia on Thursday.14This information is not literally contained\nin the training data; it is derived from relationships learned from observed information.</li>\n<li><strong>Our treatment of specific unlearning methods for removal and suppression is fairly brief.15This\nis because our purpose is to provide sufficient framing that will enable us to elicit important</strong>: Our treatment of specific unlearning methods for removal and suppression is fairly brief.15This\nis because our purpose is to provide sufficient framing that will enable us to elicit important\nconceptual gaps and limitations\u2014fundamental mismatches between unlearning motivations,\ntargets, and methods (Section 5).</li>\n<li><strong>Mismatch 2 Removal of observed information does not guarantee meaningful output suppression.</strong>: Mismatch 2 Removal of observed information does not guarantee meaningful output suppression.\n</li>\n<li><strong>Mismatch 3 Models are not equivalent to their outputs.</strong>: Mismatch 3 Models are not equivalent to their outputs.\n</li>\n<li><strong>Given the breadth of data generative-AI models ingest for training, many experts worry about</strong>: 6.1 Privacy\nGiven the breadth of data generative-AI models ingest for training, many experts worry about\nmodels revealing private information that they were trained on through their generations</li>\n<li><strong>We address some of these considerations below, organized around three broad goals that we\nobserve for unlearning-related efforts for Generative AI that pertain to privacy concerns grounded</strong>: We address some of these considerations below, organized around three broad goals that we\nobserve for unlearning-related efforts for Generative AI that pertain to privacy concerns grounded\nin regulatory frameworks: (1) data deletion (i.e., removing observed information from a model\u2019s\ntraining dataset), as well as suppression at generation time of (2) outputs that resemble personal\ninformation and (3) latent information.\n</li>\n<li><strong>In some cases, for Generative AI, data deletion is most straightforwardly implemented by re-\ntraining a model from scratch or, if applicable or feasible,26some structural-removal method</strong>: In some cases, for Generative AI, data deletion is most straightforwardly implemented by re-\ntraining a model from scratch or, if applicable or feasible,26some structural-removal method\n(Section 4.1) with the right-exercising data subject\u2019s examples removed from the training dataset.27\n</li>\n<li><strong>At first glance, as part of a response to claims in the U.S. that allege copyright infringement in\nconnection with generative-AI models and systems, it may seem appealing to attempt to use</strong>: 6.2 U.S. Copyright\nAt first glance, as part of a response to claims in the U.S. that allege copyright infringement in\nconnection with generative-AI models and systems, it may seem appealing to attempt to use\nmachine-unlearning methods to target higher-order concepts (Definition 3) that relate to creative\nexpression, as perhaps a way to operationalize notice-and-takedown requests [ 38].34However, U.S.\ncopyright is not a straightforward problem, and unlearning is not a straightforward solution.35\nWe begin with some brief background on U.S. copyright law.</li>\n<li><strong>When one looks at the processes that go into the training, deployment, and use of generative-AI</strong>: When one looks at the processes that go into the training, deployment, and use of generative-AI\nsystems, there are several places where a \u201ccopy\u201d could be made, e.g. copying the data examples\nin a training dataset within a program, training a model, and generating substantially similar\ncopies of the data examples at generation time.</li>\n<li><strong>We draw the quote above from the U.S. Executive Order on the Safe, Secure, and Trustworthy De-</strong>: We draw the quote above from the U.S. Executive Order on the Safe, Secure, and Trustworthy De-\nvelopment and Use of Artificial Intelligence; however, similar concerns and language can be found\nin a variety of legislative and policy documents, including the E.U. AI Act [ 44, Recital 110], the\nInternational Scientific Report on the Safety of Advanced AI produced by the AI Seoul Summit [ 9,\nChapter 4], and OpenAI\u2019s Preparedness Framework [ 105].</li>\n<li><strong>YesNo</strong>: YesNo\nMaybeNoRemovalNecessary?\n</li>\n<li><strong>YesNo</strong>: Su\ufb03cient?Suppressione.g., CSAM, NCII, other strictly forbidden observed informatione.g., personal data that can be processed in certain jurisdictions but not others\ne.g., synthetic CSAM, NCII deepfakes (producible from latent information + user prompts)judges, policymakers will need to make case- or domain-based decisions  about what is reasonableMaybeNoNecessary?\nSu\ufb03cient?e.g., unsafe downstream uses of otherwise innocuous or legitimate outputs judges, policymakers will need to make case- or domain-based decisions about what is reasonable suppression necessary,  see right sidee.g., synthetic CSAM, NCII deepfakes, outputs that resemble in-copyright \u201cSpiderman\u201d or real personal data (producible from latent information + user prompts)e.g., cases where the main issue is consent over use of personal data for training (for which possible model outputs might not be relevant)YesNo\nFigure 3: Following from the prior sections, four simple questions help clarify the usefulness\nof unlearning methods for removal and suppression to address policy aims for Generative AI.\n</li>\n<li><strong>For an additional example, consider a generative-AI system that includes a model trained for</strong>: For an additional example, consider a generative-AI system that includes a model trained for\nmolecular generation\u2014for suggesting formulas for new drugs and other molecules.</li>\n<li><strong>Even so, currently, a generative-AI system cannot on its own definitively determine that the</strong>: Even so, currently, a generative-AI system cannot on its own definitively determine that the\nmolecules it produces are safe for human consumption; this is the point of lab experiments\nand drug trials</li>\n<li><strong>NCII, where it is illegal or otherwise forbidden to observe this information in the model-training</strong>: Setting aside the case of the GDPR, removal from a model\u2019s\ntraining dataset (Section 4.1) may be necessary for some types of information, such as CSAM or\nNCII, where it is illegal or otherwise forbidden to observe this information in the model-training\nprocess (Section 6.3).</li>\n<li><strong>AI, it is similarly unclear if removal is generally necessary to achieve desired ends in this setting.</strong>: While this example does not directly concern Generative\nAI, it is similarly unclear if removal is generally necessary to achieve desired ends in this setting.\n</li>\n<li><strong>Instead, it may often be more important that a generative-AI model or system is unable to produce\noutputs that reflect certain observed information, certain latent information derived from it, or</strong>: Instead, it may often be more important that a generative-AI model or system is unable to produce\noutputs that reflect certain observed information, certain latent information derived from it, or\ncertain knowledge or abilities (Section 3).</li>\n<li><strong>That is, to effectively filter out some generations from being presented to end users, it may in fact\nbe necessary for the overarching generative-AI system to retain and leverage related information</strong>: That is, to effectively filter out some generations from being presented to end users, it may in fact\nbe necessary for the overarching generative-AI system to retain and leverage related information\n(Section 4).</li>\n<li><strong>Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff,</strong>: [13] Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel,\nAllan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff,\nGregory C. Allen, Jacob Steinhardt, Carrick Flynn, Se \u00b4an\u00b4O h \u00b4Eigeartaigh, Simon Beard,\nHaydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael\nPage, Joanna Bryson, Roman Yampolskiy, and Dario Amodei.</li>\n<li><strong>LLMs, 2023.</strong>: Approximate Unlearning in\nLLMs, 2023.\n</li>\n<li><strong>Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , STOC 2020,</strong>: In\nProceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing , STOC 2020,\npage 954\u2013959, New York, NY, USA, 2020.</li>\n<li><strong>Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks, 2015.</strong>: An Empirical\nInvestigation of Catastrophic Forgetting in Gradient-Based Neural Networks, 2015.\n</li>\n<li><strong>The Next Generation, 2023.</strong>: AI and Law:\nThe Next Generation, 2023.\n</li>\n<li><strong>Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-\nBurger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass,\nOliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao,\nAriel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika,\nZifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih,\nKemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis,\nAlex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen\nFitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu\nWang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexander Wang, and Dan Hendrycks.</strong>: [81] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D.\nLi, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, Gabriel Mukobi, Nathan Helm-\nBurger, Rassin Lababidi, Lennart Justen, Andrew B. Liu, Michael Chen, Isabelle Barrass,\nOliver Zhang, Xiaoyuan Zhu, Rishub Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao,\nAriel Herbert-Voss, Cort B. Breuer, Samuel Marks, Oam Patel, Andy Zou, Mantas Mazeika,\nZifan Wang, Palash Oswal, Weiran Lin, Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih,\nKemper Talley, John Guan, Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis,\nAlex Levinson, Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen\nFitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay Varadharajan, Ruoyu\nWang, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexander Wang, and Dan Hendrycks.\n</li>\n<li><strong>InIEEE INFOCOM 2022 - IEEE Conference on Computer Communications , page</strong>: InIEEE INFOCOM 2022 - IEEE Conference on Computer Communications , page\n1749\u20131758.</li>\n<li><strong>Eight Methods to Evaluate Robust Unlearning in LLMs, 2024.</strong>: Eight Methods to Evaluate Robust Unlearning in LLMs, 2024.\n</li>\n<li><strong>Methods for Machine Unlearning, 2020.</strong>: Descent-to-Delete: Gradient-Based\nMethods for Machine Unlearning, 2020.\n</li>\n<li><strong>The White House.</strong>: The White House.\n</li>\n<li><strong>Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy</strong>: [108] USVSN Sai Prashanth, Alvin Deng, Kyle O\u2019Brien, Jyothir S V au2, Mohammad Aflah Khan,\nJaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy\nKe, Katherine Lee, and Naomi Saphra.</li>\n<li><strong>Working Paper.</strong>: Working Paper.\n</li>\n<li><strong>Verification of Machine Unlearning, 2020.</strong>: Towards Probabilistic\nVerification of Machine Unlearning, 2020.\n</li>\n<li><strong>The White House.</strong>: [130] The White House.</li>\n<li><strong>The White House.</strong>: The White House.\n</li>\n<li><strong>Kairan Zhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan,\nLisheng Sun Hosoya, Sergio Escalera, Gintare Karolina Dziugaite, Peter Triantafillou, and</strong>: [131] Eleni Triantafillou, Peter Kairouz, Fabian Pedregosa, Jamie Hayes, Meghdad Kurmanji,\nKairan Zhao, Vincent Dumoulin, Julio Jacques Junior, Ioannis Mitliagkas, Jun Wan,\nLisheng Sun Hosoya, Sergio Escalera, Gintare Karolina Dziugaite, Peter Triantafillou, and\nIsabelle Guyon.</li>\n<li><strong>A Survey, 2023.</strong>: Machine Unlearning:\nA Survey, 2023.\n</li>\n<li><strong>A Growth of unlearning papers over time\nWe provide some cursory evidence to support that there has been massive growth of machine</strong>: A Growth of unlearning papers over time\nWe provide some cursory evidence to support that there has been massive growth of machine\nunlearning papers in the last few years (Figure 4).</li>\n<li><strong>Year04080120160200240280320360400440Number of papersGPT-2 T5 DALL-EPaLMSDChatGPTClaudeUnlearning papers posted to arXiv over time</strong>: 2017 2018 2019 2020 2021 2022 2023 2024\nYear04080120160200240280320360400440Number of papersGPT-2 T5 DALL-EPaLMSDChatGPTClaudeUnlearning papers posted to arXiv over time\nFigure 4: We scrape all papers that match unlearn* ormodel forgetting from arXiv and plot\ntheir counts over time, as of December 4, 2024.</li>", "entity_table": [{"name": "james grimmelmann,,vitaly", "frequency": 1, "pages": "1"}, {"name": "amy cyphertmark lemleydanah boydjennifer wortman", "frequency": 1, "pages": "1"}, {"name": "miles brundage david bauseth neelabigail z.", "frequency": 1, "pages": "1"}, {"name": "michael kearns", "frequency": 1, "pages": "25"}, {"name": "jacob steinhardt", "frequency": 5, "pages": "25, 25, 28, 29, 31"}, {"name": "john schulman", "frequency": 2, "pages": "25, 28"}, {"name": "andrew d. selbst", "frequency": 1, "pages": "25"}, {"name": "james glass", "frequency": 1, "pages": "25"}, {"name": "lucas bourtoule", "frequency": 1, "pages": "25"}, {"name": "christopher choquette-choo", "frequency": 1, "pages": "25"}, {"name": "david lie", "frequency": 1, "pages": "25"}, {"name": "hannah brown", "frequency": 1, "pages": "25"}, {"name": "jack clark", "frequency": 2, "pages": "25, 32"}, {"name": "heather roff", "frequency": 1, "pages": "25"}, {"name": "michael page", "frequency": 1, "pages": "25"}, {"name": "matthew jagielski", "frequency": 3, "pages": "26, 31, 32"}, {"name": "hannah washington", "frequency": 2, "pages": "26, 33"}, {"name": "emily sheng", "frequency": 2, "pages": "26, 33"}, {"name": "james grimmelmann", "frequency": 6, "pages": "26, 27, 27, 28, 29, 29"}, {"name": "karen levy", "frequency": 2, "pages": "26, 26"}, {"name": "christopher de sa", "frequency": 4, "pages": "26, 26, 27, 27"}, {"name": "benjamin laufer", "frequency": 1, "pages": "26"}, {"name": "christopher callison-burch", "frequency": 1, "pages": "27"}, {"name": "christopher a. choquette-choo", "frequency": 3, "pages": "27, 31, 31"}, {"name": "david mimno", "frequency": 1, "pages": "27"}, {"name": "jack m. balkin", "frequency": 1, "pages": "27"}, {"name": "jonathan frankle", "frequency": 2, "pages": "27, 28"}, {"name": "elizabeth joh", "frequency": 1, "pages": "27"}, {"name": "gautam kamath", "frequency": 1, "pages": "27"}, {"name": "gabriel ilharco", "frequency": 1, "pages": "27"}, {"name": "noah smith", "frequency": 1, "pages": "27"}, {"name": "michael feffer", "frequency": 1, "pages": "28"}, {"name": "jonathan berant", "frequency": 1, "pages": "28"}, {"name": "james zou", "frequency": 1, "pages": "28"}, {"name": "michael tontchev", "frequency": 1, "pages": "29"}, {"name": "james diffenderfer", "frequency": 1, "pages": "29"}, {"name": "kevin klyman", "frequency": 2, "pages": "29, 34"}, {"name": "alexander pan", "frequency": 1, "pages": "30"}, {"name": "daniel berrios", "frequency": 1, "pages": "30"}, {"name": "alice gatti", "frequency": 2, "pages": "30, 32"}, {"name": "justin d. li", "frequency": 1, "pages": "30"}, {"name": "gabriel mukobi", "frequency": 1, "pages": "30"}, {"name": "nathan helm- burger", "frequency": 1, "pages": "30"}, {"name": "andrew b. liu", "frequency": 1, "pages": "30"}, {"name": "michael chen", "frequency": 1, "pages": "30"}, {"name": "justin tienken-harder", "frequency": 1, "pages": "30"}, {"name": "kevin y. shih", "frequency": 1, "pages": "30"}, {"name": "john guan", "frequency": 1, "pages": "30"}, {"name": "david campbell", "frequency": 1, "pages": "30"}, {"name": "kevin m. esvelt", "frequency": 2, "pages": "30, 32"}, {"name": "alexander wang", "frequency": 1, "pages": "30"}, {"name": "jack hessel", "frequency": 1, "pages": "30"}, {"name": "dylan hadfield-menell", "frequency": 1, "pages": "30"}, {"name": "michael mathioudakis", "frequency": 1, "pages": "30"}, {"name": "daniel mcduff", "frequency": 1, "pages": "30"}, {"name": "josua benjamin", "frequency": 1, "pages": "30"}, {"name": "joseph lindley", "frequency": 1, "pages": "30"}, {"name": "kevin meng", "frequency": 1, "pages": "30"}, {"name": "david bau", "frequency": 1, "pages": "30"}, {"name": "eric mitchell", "frequency": 1, "pages": "31"}, {"name": "christopher d manning", "frequency": 1, "pages": "31"}, {"name": "jonathan hayase", "frequency": 1, "pages": "31"}, {"name": "eric wallace", "frequency": 1, "pages": "31"}, {"name": "alexander h miller", "frequency": 1, "pages": "31"}, {"name": "jacob ray fuehne", "frequency": 1, "pages": "31"}, {"name": "david smith", "frequency": 1, "pages": "31"}, {"name": "emily allaway", "frequency": 1, "pages": "32"}, {"name": "hannah rashkin", "frequency": 1, "pages": "32"}, {"name": "emily h. soice", "frequency": 1, "pages": "32"}, {"name": "michael specter", "frequency": 1, "pages": "32"}, {"name": "daniel j. solove", "frequency": 1, "pages": "32"}, {"name": "david marco sommer", "frequency": 1, "pages": "32"}, {"name": "justin wang", "frequency": 1, "pages": "32"}, {"name": "christopher j. williams", "frequency": 1, "pages": "33"}, {"name": "meera desai", "frequency": 1, "pages": "33"}, {"name": "emily corvi", "frequency": 1, "pages": "33"}, {"name": "jennifer wortman vaughan", "frequency": 1, "pages": "33"}, {"name": "matthew vogel", "frequency": 1, "pages": "33"}, {"name": "noah a. smith", "frequency": 1, "pages": "33"}, {"name": "jason wei", "frequency": 1, "pages": "33"}, {"name": "iason gabriel", "frequency": 1, "pages": "33"}, {"name": "william isaac", "frequency": 1, "pages": "33"}, {"name": "daniel wilf-townsend", "frequency": 1, "pages": "33"}, {"name": "jonathan zittrain", "frequency": 1, "pages": "34"}], "date_table": [{"date": "2002/10/14", "pages": "28"}, {"date": "2024/06/03", "pages": "31"}, {"date": "2024/09/12", "pages": "31"}, {"date": "2024/01/12", "pages": "32"}, {"date": "2023/05/19", "pages": "33"}, {"date": "2023/10/30", "pages": "33"}, {"date": "2024/03/21", "pages": "33"}, {"date": "2024/07/29", "pages": "33"}], "title_position_table": [{"title_position": "developer", "pages": "10, 10, 23, 24"}, {"title_position": "President", "pages": "33"}], "organization_table": [{"organization": "Lee\u22c61,4", "pages": "1"}, {"organization": "Democracy & Technology6Princeton", "pages": "1"}, {"organization": "Law School13Cornell University14West Virginia University", "pages": "1"}, {"organization": "ML", "pages": "1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 4, 5, 7, 7, 7, 8, 9, 10, 10, 21, 22, 22, 23, 23, 23, 23, 23, 23"}, {"organization": "General Data Protection Regulation", "pages": "2, 31"}, {"organization": "Sections 6 & 7", "pages": "2"}, {"organization": "CSAM", "pages": "2, 21, 21, 21, 22"}, {"organization": "AI", "pages": "3, 14, 21, 23, 23, 32"}, {"organization": "Data Protection Regulation", "pages": "4"}, {"organization": "linear", "pages": "4"}, {"organization": "Cauwenberghs", "pages": "5"}, {"organization": "Claude", "pages": "5"}, {"organization": "Sections 4 & 5", "pages": "6"}, {"organization": "LLM", "pages": "7, 7"}, {"organization": "Sections 1 & 2.2", "pages": "8"}, {"organization": "Sections 2.1 & 3", "pages": "8"}, {"organization": "Sections 4.2 & 5", "pages": "9"}, {"organization": "Sections 3 & 4.1", "pages": "10"}, {"organization": "WMDP", "pages": "11, 11, 20"}, {"organization": "Spiderman", "pages": "12"}, {"organization": "Sections 4 & 7.2", "pages": "12"}, {"organization": "the California Consumer Privacy Act", "pages": "14"}, {"organization": "the Virginia Consumer Data Protection Act", "pages": "14"}, {"organization": "Mismatches 2 & 3", "pages": "15, 22"}, {"organization": "the Court of Justice of the European Union", "pages": "15"}, {"organization": "CJEU", "pages": "15, 15, 22"}, {"organization": "Google", "pages": "15"}, {"organization": "the European Union", "pages": "15, 27, 27, 31"}, {"organization": "Chouldechova", "pages": "15"}, {"organization": "Importantly", "pages": "16"}, {"organization": "NCII", "pages": "16, 20, 21, 21, 22, 22"}, {"organization": "Disney", "pages": "17"}, {"organization": "PDF", "pages": "17"}, {"organization": "CommonCanvas", "pages": "18, 18, 18, 28"}, {"organization": "Creative Commons", "pages": "18, 18"}, {"organization": "Disney World", "pages": "18"}, {"organization": "Artificial Intelligence", "pages": "19, 33"}, {"organization": "E.U", "pages": "19"}, {"organization": "the International Scientific Report", "pages": "19"}, {"organization": "the Safety of Advanced AI", "pages": "19"}, {"organization": "the AI Seoul Summit", "pages": "19"}, {"organization": "OpenAI \u2019 s Preparedness Framework", "pages": "19"}, {"organization": "AI Seoul Summit", "pages": "19, 25"}, {"organization": "GDPR", "pages": "22"}, {"organization": "Sections 2.3 & 2.4", "pages": "22, 23"}, {"organization": "RLHF", "pages": "22"}, {"organization": "Sections 2.1 & 2.2", "pages": "23"}, {"organization": "Meta", "pages": "24"}, {"organization": "the \u201c Fallacy of the Almost-General-Purpose Computer", "pages": "24"}, {"organization": "Evaluating Generative AI Systems", "pages": "25"}, {"organization": "Hype", "pages": "25"}, {"organization": "The K & L Gates Initiative in Ethics", "pages": "25"}, {"organization": "CMU", "pages": "25"}, {"organization": "The GenLaw Center", "pages": "25"}, {"organization": "Georgetown Institute for Technology Law & Policy", "pages": "25"}, {"organization": "the Center for Democracy & Technology", "pages": "25"}, {"organization": "Concrete Problems", "pages": "25"}, {"organization": "AI Safety", "pages": "25"}, {"organization": "LLC", "pages": "25"}, {"organization": "MGA Ent", "pages": "25"}, {"organization": "Google , Inc.", "pages": "25"}, {"organization": "Big Data \u2019 s Disparate Impact", "pages": "25"}, {"organization": "Yonatan Belinkov", "pages": "25, 30"}, {"organization": "al .", "pages": "25"}, {"organization": "the Safety of Advanced AI : Interim Report", "pages": "25"}, {"organization": "Data Leakage and Unlearning for Law", "pages": "25"}, {"organization": "Lucas Bourtoule", "pages": "25"}, {"organization": "Owain Evans", "pages": "25"}, {"organization": "California State Legislature", "pages": "25, 26"}, {"organization": "|Digital Democracy", "pages": "26"}, {"organization": "Campbell", "pages": "26"}, {"organization": "Acuff-Rose Music", "pages": "26"}, {"organization": "Hanna Wallach", "pages": "26, 33"}, {"organization": "Zero-Shot Machine Unlearning", "pages": "26"}, {"organization": "ISSN", "pages": "26, 29"}, {"organization": "Aengus Lynch", "pages": "26, 30"}, {"organization": "Automated Circuit Discovery for Mechanistic Interpretability", "pages": "26"}, {"organization": "Generative-AI Systems", "pages": "26"}, {"organization": "Accurate", "pages": "26"}, {"organization": "Accuracy-Efficiency Trade-Offs and Accountability in Distributed ML Systems", "pages": "26"}, {"organization": "Equity and Access", "pages": "26"}, {"organization": "the AAAI Conference on Artificial Intelligence", "pages": "27"}, {"organization": "Court of Justice", "pages": "27"}, {"organization": "Google Inc.", "pages": "27"}, {"organization": "Commission", "pages": "27"}, {"organization": "Data Orders", "pages": "27"}, {"organization": "European Union", "pages": "27"}, {"organization": "EU", "pages": "27, 27, 27, 27, 27, 27, 27, 27, 27, 31"}, {"organization": "the European Parliament", "pages": "27"}, {"organization": "the Council of 13 June 2024", "pages": "27"}, {"organization": "EC", "pages": "27"}, {"organization": "Official Journal", "pages": "27"}, {"organization": "Digital", "pages": "28"}, {"organization": "The Challenges of Data Protection Beyond Borders", "pages": "28"}, {"organization": "German Law Journal", "pages": "28"}, {"organization": "Security Theater", "pages": "28"}, {"organization": "Feist Publications", "pages": "28"}, {"organization": "Rural Telephone Service Company", "pages": "28"}, {"organization": "Luciano Floridi", "pages": "28"}, {"organization": "Delete Culture", "pages": "28"}, {"organization": "Philosophy & Technology", "pages": "28"}, {"organization": "Transformer Feed-Forward Layers Are Key-Value Memories", "pages": "28"}, {"organization": "the 33rd International Conference on Neural Information Processing Systems", "pages": "28"}, {"organization": "Red Hook", "pages": "28"}, {"organization": "Curran Associates Inc.", "pages": "28"}, {"organization": "An Open Diffusion Model Trained with Creative-Commons Images", "pages": "28"}, {"organization": "Literate Robots", "pages": "28"}, {"organization": "Iowa Law Review", "pages": "28"}, {"organization": "CoRR", "pages": "28"}, {"organization": "abs/1911.03030", "pages": "28"}, {"organization": "Mechanistic Interpretability", "pages": "28"}, {"organization": "ML Safety", "pages": "28"}, {"organization": "Guardian", "pages": "28"}, {"organization": "Mariarosaria Taddeo", "pages": "29"}, {"organization": "AI Through Machine Unlearning", "pages": "29"}, {"organization": "Science and Engineering Ethics", "pages": "29"}, {"organization": "Davide Testuggine", "pages": "29"}, {"organization": "Input-Output Safeguard for Human-AI Conversations", "pages": "29"}, {"organization": "Dongkeun Yoon", "pages": "29"}, {"organization": "Sungmin Cha", "pages": "29"}, {"organization": "Challenges and Opportunities", "pages": "29"}, {"organization": "Computer Law & Security Review", "pages": "29"}, {"organization": "Foundation Models", "pages": "29"}, {"organization": "NLP", "pages": "29"}, {"organization": "Findings of the Association for Computa- tional Linguistics", "pages": "29"}, {"organization": "IJCNLP-AACL 2023", "pages": "29"}, {"organization": "Eleni Triantafillou", "pages": "29"}, {"organization": "Harvard Business School", "pages": "29"}, {"organization": "the Generative-AI Supply Chain", "pages": "29"}, {"organization": "Texas Law Review", "pages": "29"}, {"organization": "Nathan Helm- Burger", "pages": "30"}, {"organization": "Palash Oswal", "pages": "30"}, {"organization": "Neuron Masking", "pages": "30"}, {"organization": "Federated Learning", "pages": "30"}, {"organization": "IEEE Press", "pages": "30"}, {"organization": "AI Meta Llama Team", "pages": "30"}, {"organization": "Prithviraj Ammanabrolu", "pages": "30"}, {"organization": "Yejin Choi", "pages": "30, 30, 32"}, {"organization": "Linear Models", "pages": "30"}, {"organization": "Zachary C. Lipton", "pages": "30"}, {"organization": "Danish Contractor", "pages": "30"}, {"organization": "GPT", "pages": "30"}, {"organization": "Neural Information Processing Systems", "pages": "30"}, {"organization": "Contextual Integrity Theory", "pages": "30"}, {"organization": "Antoine Bosselut", "pages": "31"}, {"organization": "Le Song", "pages": "31"}, {"organization": "the 39th International Conference on Machine Learning", "pages": "31"}, {"organization": "Machine Learning Research", "pages": "31"}, {"organization": "Comprehensive Mechanistic Interpretability", "pages": "31"}, {"organization": "Explainer & Glossary", "pages": "31"}, {"organization": "Language Models", "pages": "31, 31"}, {"organization": "arXiv:2311.17035", "pages": "31"}, {"organization": "White House", "pages": "31"}, {"organization": "The White House", "pages": "31, 33, 33"}, {"organization": "arXiv:1909.01066", "pages": "31"}, {"organization": "USVSN", "pages": "31"}, {"organization": "AAAI", "pages": "32"}, {"organization": "Are Emergent Abilities of Large Language Models", "pages": "32"}, {"organization": "Mirage", "pages": "32"}, {"organization": "the Landscape of Machine Unlearning : A Comprehensive Survey", "pages": "32"}, {"organization": "Yarin Gal", "pages": "32"}, {"organization": "Nianwen Si", "pages": "32"}, {"organization": "Privacy", "pages": "32"}, {"organization": "Working Paper", "pages": "32"}, {"organization": "Digital Forgery", "pages": "32"}, {"organization": "Investigating Data Replication in Diffusion Models", "pages": "32"}, {"organization": "Technical Report NIST AI", "pages": "32"}, {"organization": "National Institute of Standards and Technology", "pages": "32"}, {"organization": "Tarun Suresh", "pages": "32"}, {"organization": "Mantas Mazeika", "pages": "32"}, {"organization": "Limitations", "pages": "32"}, {"organization": "Societal Impact of Large Language Models", "pages": "32"}, {"organization": "AI Safety Summit", "pages": "33"}, {"organization": "U.S. Department of Commerce", "pages": "33"}, {"organization": "Department of Commerce Announces New Guid-", "pages": "33"}, {"organization": "UT News", "pages": "33"}, {"organization": "Sociotechnical Safety Evaluation of Generative AI Systems", "pages": "33"}, {"organization": "AI Models Forget Undesirable Data Hurts Their Performance", "pages": "33"}, {"organization": "TechCrunch", "pages": "33"}, {"organization": "International Joint Conferences on Artificial Intelligence Organization", "pages": "34"}, {"organization": "AI Ethics", "pages": "34"}, {"organization": "Benchmark Machine Unlearning for Diffusion Models", "pages": "34"}, {"organization": "The Future of the Internet", "pages": "34"}, {"organization": "Yale University Press , USA", "pages": "34"}, {"organization": "Adversarial Perspective on Machine Unlearning for AI Safety", "pages": "34"}, {"organization": "PaLM", "pages": "35"}, {"organization": "Stable Diffusion ( SD", "pages": "35"}], "url_table": [{"url": "www.anthropic", "pages": "25"}, {"url": "www.anthropic.com/claude", "pages": "25"}, {"url": "www.law.cornell.edu/uscode/text/17/102", "pages": "27"}, {"url": "www.law.cornell.edu/uscode/text/17/107", "pages": "27"}, {"url": "www.law.cornell.edu/uscode/text/17/506", "pages": "27"}, {"url": "www.law.cornell.edu/uscode/text/17/503", "pages": "27"}, {"url": "www.law.cornell.edu/uscode/text/17/504", "pages": "27"}, {"url": "www.law.cornell.edu/uscode/text/17/512", "pages": "27"}, {"url": "www.theguardian.com/technology/2017/apr/05/facebook-tools-revenge-porn", "pages": "28"}, {"url": "www.sciencedirect.com/science/article/pii/S026736492300095X", "pages": "29"}, {"url": "www.politico.com/newsletters/digital-future-daily/2024/06/03/ai-", "pages": "31"}, {"url": "www.neelnanda.io/mechanistic-interpretability/glossary", "pages": "31"}, {"url": "www.whitehouse.gov/ostp/news-updates/2024/09/12/white-house-announces-", "pages": "31"}, {"url": "www.axios.com/2024/01/12/ai-forget-unlearn-data-privacy", "pages": "32"}, {"url": "www.biorxiv.org/content/early/2023/05/19/2022.11.21.517405", "pages": "33"}, {"url": "www.whitehouse.gov/briefing-", "pages": "33"}, {"url": "www.gov.uk/government/news/iconic-bletchley-park-to-host-uk-", "pages": "33"}, {"url": "www.commerce.gov/news/press-releases/2024/07/department-commerce-", "pages": "33"}], "email_table": [], "phone_number_table": [{"phone_number": "555-123-4567", "frequency": 3, "pages": "7, 7, 7"}, {"phone_number": "3531146", "frequency": 2, "pages": "25, 26"}, {"phone_number": "3534642", "frequency": 1, "pages": "25"}, {"phone_number": "1556-6013", "frequency": 1, "pages": "26"}, {"phone_number": "3265506", "frequency": 1, "pages": "26"}, {"phone_number": "3465416", "frequency": 1, "pages": "26"}, {"phone_number": "3483289", "frequency": 1, "pages": "26"}, {"phone_number": "3533150", "frequency": 1, "pages": "26"}, {"phone_number": "9796721", "frequency": 2, "pages": "30, 30"}, {"phone_number": "4933011", "frequency": 1, "pages": "33"}, {"phone_number": "0300124872", "frequency": 1, "pages": "34"}], "address_table": [{"address": "810 papers st", "frequency": 1, "pages": "35"}]}